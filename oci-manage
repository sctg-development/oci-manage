#!/bin/bash
#
# oci-manage -- A library providing functions for building and managing
# a bare metal Kubernetes cluster on Oracle OCI
#
# Feel free to contribute to this project at:
#    https://github.com/eltorio/oci-manage
#
# Copyright 2023-2024 Ronan Le Meillat.
#
# oci-manage is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# oci-manage is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.

# You should have received a copy of the GNU Affero General Public License
# along with oci-manage.  If not, see <https://www.gnu.org/licenses/agpl-3.0.html>.
#
#### default values (real are in ./oci-manage-config.sh )
AZURE_CERT_MANAGER_SP_APP_ID="4330169c-062c-4418-983d-0ae202fa7918"
AZURE_CERT_MANAGER_SP_PASSWORD="pohsh5ahl1quoong6aa0iug8toogh3aengup0cha7E"
AZURE_DNS_ID="/subscriptions/6ac7c0a9-95e3-4c76-81df-afdcb22f1b46/resourceGroups/ResourceName/providers/Microsoft.Network/dnszones/example.org"
AZURE_DNS_ZONE=example.org
AZURE_SUBSCRIPTION_ID="553f8fa4-6c82-4de7-888b-8981d757fe31"
AZURE_TENANT_ID="3ea6cdac-a005-4d3e-bff7-1a5dbfa4716b"
OCI=~/bin/oci
ARM_SRC=master
CA_COMPANY="my"
CRT_K8S="company.pem"
CF_API_KEY="21257c46300680ad3891126bd6d9896ee2c13181"
CF_API_EMAIL="user@example.org"
CF_DOMAINS=("example.org" "example.com")
CLUSTER_AS=4201234567
CLUSTER_MEMBERS='node2 node3 node4 node5 node6 node7 node8'
CILIUM_CLUSTER_POOL_IPV4_KUBERNETES_POD_CIDR=172.24.0.0/13
CILIUM_CLUSTER_POOL_IPV6_KUBERNETES_POD_CIDR=fd62:83ce:1203:1234::/64
CILIUM_VERSION="v1.14.6" #"v1.13.6"
CLUSTER_HA_FQDN=ha.fqdn.com
CONTROL_PLANE_INTERNAL_ADDRESS=master.private.fqdn.com
ROUTING_MEMBER='oci-router'
#set if needed to access service IP as EXTERNAL_CLUSTER_ZONE
ETCDCTL_PKI="--cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key"
EXTERNAL_CLUSTER_ZONE=cluster.external
CONTROL_PLANE_IP=10.0.253.75
CONTROL_PLANE_LOCAL=master
CONTROL_PLANE_PORT=6443
DNS_PUBLIC_ZONE=example.org
DOCKER_RUNTIME=containerd
CILIUM_LB_IP_POOL=172.31.255.0/24
KUBE_PROXY_REPLACEMENT=true
KUBERNET_MAJOR_VERSION=1.29
HAPROXY_INGRESS_CONTROLLER_VERSION="1.10.8"
LETSENCRYPT_USER="user@example.com"
OCI_DB=~/oci.json
OCI_FINGERPRINT="52:c3:b9:6a:2d:6b:08:c9:2d:31:fc:68:51:0d:75:aa"
OCI_PRIVATE_KEY="LS0tLS1CRUdJTiB………VZBVEUgS0VZLS0tLS0K"
OCI_REGION="eu-marseille-1"
OCI_TENANCY="ocid1.tenancy.oc1..9d843b1968e162816404c78f2afc854b15fd3d996008f9827e3028ce2e96e3c3"
OCI_USER="ocid1.user.oc1..9d843b1968e162816404c78f2afc854b15fd3d996008f9827e3028ce2e96e3c3"
KUBERNETES_POD_CIDR=172.28.0.0/14
PRIVATE_INTERFACE_FILE="60-private-interface.yaml"
KUBERNETES_SERVICE_CIDR=172.24.0.0/14
MASTER_ADMIN_PASSWORD='strongpassword'
MASTER_ADMIN='admin'
X86_64_SRC=node4
#MASTER_ADMIN MASTER_ADMIN_PASSWORD are also used for hubble-ui dashboard
DOCKER_REGISTRY_HOST=docker-registry.local
DOCKER_REGISTRY_UI_HOST=docker-registry-ui.local
DOCKER_REGISTRY_PERSISTENT=false
DOCKER_REGISTRY_USER=ociregistry
DOCKER_REGISTRY_PASSWORD=oon.aing3eiP
DOCKER_REGISTRY_UI_DNS_NAMES=("docker-registry-ui.kube-system.svc.cluster.local")
HUBBLE_DASHBOARD_DNS_NAMES=("hubble-ui.fqdn" "hubble-ui.kube-system.svc.cluster.local")
HELM_DASHBOARD_DNS_NAMES=("helm.fqdn" "helm.helm-dashboard.svc.cluster.local")
ROOT_CA="LS0tLS1CRUdJTiBDRVJ198Z1290ZNKJNDQSQ0FURS0tLS0tCk1JSUZteRFLS0tLS0K"
TRAEFIK_DASHBOARD_DNS_NAMES=("traefik-dashboard.fqdn" "traefik.kube-traefik.svc.cluster.local")
# cat ~/pki/ca.crt | base64 | tr -d '\n'
CA="LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZteRFLS0tLS0K"
# cat ~/pki/ca.key | base64 | tr -d '\n'
CA_KEY="LS0tLS1C0tLQo="
DASHBOARD_DNS_NAMES=("dashboard.fqdn" "dashboard.kube-dashboard.svc.cluster.local")
LONGHORN_DASHBOARD_DNS_NAMES=("longhorn.fqdn" "longhorn-frontend.kube-system.svc.cluster.local")
STORAGE_BACKEND="longhorn"
#STORAGE_BACKEND="openebs"
STORAGE_LOCATION="/storage"
STORAGE_REPLICAS=2
LONGHORN_VERSION="1.5.3"
# LONGHORN_VERSION="1.4.3"
LONGHORN_CA_ISSUER=$CA_COMPANY-ca-issuer
MONITORING_NAMESPACE=monitoring
GRAFANA_AGENT_VERSION="v0.32.1"
GATEWAY_API_VERSION="v0.8.1"
KUBE_STATE_METRICS_VERSION="v2.10.0"
GRAFANA_CLUSTER_NAME="KubernetesOCI"
GRAFANA_PROMETHEUS_USERNAME="123456"
GRAFANA_PROMETHEUS_PASSWORD="MTFhM2UyMjkwODQzNDliYzI1ZDk3ZTI5MzkzY2VkMWQgIC0K"
GRAFANA_LOGS_USERNAME="654321"
GRAFANA_LOGS_PASSWORD="MTFhM2UyMjkwODQzNDliYzI1ZDk3ZTI5MzkzY2VkMWQgIC0K"

SSH_SUPER_PRIVATE_KEY="-----BEGIN OPENSSH PRIVATE KEY-----\n\
b3BlbnNzaC1rZXktdjEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAaAAAABNlY2RzYS\n\
b3BlbnNzaC1rZXktdjEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAaAAAABNlY2RzYS\n\
b3BlbnNzaC1rZXktdjEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAaAAAABNlY2RzYS\n\
b3BlbnNzaC1rZXktdjEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAaAAAABNlY2RzYS\n\
b3BlbnNzaC1rZXktdjEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAaAAAABNlY2RzYS\n\
c0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgMEBQYH\n\
-----END OPENSSH PRIVATE KEY-----"
S3PATH="s3/master"
SUPER_PRIVATE_USER=ubuntu
SSH_ROOT_KEYS=("ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYT124UWsqs1kzErc9bIOpA= me@myhost")
SSH_ROOT_KEYS+=("ecdsa-sha2-nistp256 AAAAE2VjZHNhL6780iaBp9aV+ZzlLsF6tm3Bm0= you@ahost")
WG_DATABASE="./wg"
WG_TABLE="main"
WG_MARK="0x672"
WEBLATE_ADMIN=admin
WEBLATE_PASSWORD=password
WEBLATE_EMAIL=admin@example.org
WEBLATE_EMAIL_USER=user@example.org
WEBLATE_EMAIL_PASSWORD=password
WEBLATE_EMAIL_SENDER="weblate@example.org"
WEBLATE_EMAIL_SENDER_PASSWORD="goodpassword"
WEBLATE_EMAIL_SENDER_HOST="smtp.example.org"
WEBLATE_EMAIL_SENDER_PORT="587"
WEBLATE_REGISTRATION_EMAIL_MATCH='r".*"'
WEBLATE_GPG_IDENTITY="weblate@weblate.org"
############### end of default values

. ~/oci-manage-config.sh

CLUSTER_DOMAIN=${CLUSTER_HA_FQDN#${CLUSTER_HA_FQDN%%.*}.}
MASTER_ADMIN_PASSWORD_CRYPTB64=$(openssl passwd -1 $MASTER_ADMIN_PASSWORD | base64)
K8S_CLUSTER="$CLUSTER_MEMBERS $CONTROL_PLANE_LOCAL"
K8S_CLUSTER_LONG="$CLUSTER_MEMBERS $CONTROL_PLANE_INTERNAL_ADDRESS"
DASHBOARD_DNS_NAMES_JS=$(
  RES=""
  for NAME in "${DASHBOARD_DNS_NAMES[@]}"; do RES+="$NAME,"; done
  echo "[${RES:0:-1}]"
)
TRAEFIK_DASHBOARD_DNS_NAMES_JS=$(
  RES=""
  for NAME in "${TRAEFIK_DASHBOARD_DNS_NAMES[@]}"; do RES+="$NAME,"; done
  echo "[${RES:0:-1}]"
)
HUBBLE_DASHBOARD_DNS_NAMES_JS=$(
  RES=""
  for NAME in "${HUBBLE_DASHBOARD_DNS_NAMES[@]}"; do RES+="$NAME,"; done
  echo "[${RES:0:-1}]"
)
HELM_DASHBOARD_DNS_NAMES_JS=$(
  RES=""
  for NAME in "${HELM_DASHBOARD_DNS_NAMES[@]}"; do RES+="$NAME,"; done
  echo "[${RES:0:-1}]"
)
LONGHORN_DASHBOARD_DNS_NAMES_JS=$(
  RES=""
  for NAME in "${LONGHORN_DASHBOARD_DNS_NAMES[@]}"; do RES+="$NAME,"; done
  echo "[${RES:0:-1}]"
)
DOCKER_REGISTRY_UI_DNS_NAMES_JS=$(
  RES=""
  for NAME in "${DOCKER_REGISTRY_UI_DNS_NAMES[@]}"; do RES+="$NAME,"; done
  echo "[${RES:0:-1}]"
)
if [[ $DOCKER_RUNTIME == "containerd" ]]; then
  CRI_SOCKET="unix:///run/containerd/containerd.sock"
else
  CRI_SOCKET="unix:///run/cri-dockerd.sock"
fi
function create_admin_user_with_key() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage create_admin_user_with_key username password"
  else
    USERNAME="$1"
    PASSWORD="$2"
    sudo useradd -m -s /bin/bash -p "$PASSWORD" "$USERNAME"
    sudo usermod -aG sudo "$USERNAME"
    sudo -u "$USERNAME" sh -c "cd &&\
                                   mkdir -p .ssh &&\
                                   chmod go-rwx .ssh &&\
                                   ssh-keygen -q -t ecdsa -f ~/.ssh/id_ecdsa -N ''"
  fi
}

function init_new_host() {
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_new_host host username password"
  else
    _ssh_super_secret_key
    HOST="$1"
    USERNAME="$2"
    PASSWORD="$3"
    init_create_admin_user_with_key "$HOST" "$USERNAME" "$PASSWORD"
    init_allow_keys_for_root "$HOST" "$USERNAME" "$PASSWORD"
    init_deploy_admin_keys_to_admin_user "$HOST" "$USERNAME"
    init_set_ramdisk "$HOST"
    init_set_iptables "$HOST"
    if [[ $DOCKER_RUNTIME == "containerd" ]]; then
      init_install_cri_containerd "$HOST"
    else
      init_install_cri_docker "$HOST"
    fi
    init_install_software "$HOST" "$USERNAME"
    init_install_fill_memory "$HOST"
    echo "###################################################"
    echo "# host pre-installed"
    echo "# you may want to finish the network installation"
    echo "# pehaps the good tool is"
    echo "#     init_create_private_interface "$HOST" "MACADDRESS" "PRIVATE_IP_ADDRESS" "
    echo "#     init_set_hostname "$HOST" "PRIVATE_HOST_NAME""
    echo "# next add "$HOST" to the CLUSTER_MEMBERS in the config file"
    echo "# add the host to /etc/hosts and deploy it with:"
    echo "#      cluster_deploy_hosts"
    echo "# you may want to recreate all private interfaces with:"
    echo "# cluster_recreate_private_interface"
    echo "# use cluster_init_create_member to finally add the new member to the cluster"
  fi
}

function init_set_hostname() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_set_hostname host privatehostname"
  else
    HOST=$1
    PRIVATE=$2
    ssh -4 root@$HOST "hostnamectl hostname $PRIVATE"
  fi
}
function init_allow_keys_for_root() {
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_create_admin_user_with_key host username password"
  else
    _ssh_super_secret_key
    HOST="$1"
    USERNAME="$2"
    PASSWORD="$3"
    TEST=TEST
    ssh -4 $SUPER_PRIVATE_USER@$HOST -i ~/.ssh/id_super_private "sudo sed -i.bak s/#PermitRootLogin/PermitRootLogin/ /etc/ssh/sshd_config && sudo systemctl restart ssh"
    _CMD_="echo '' > ~/.ssh/authorized_keys"
    for KEY in "${SSH_ROOT_KEYS[@]}"; do
      _CMD_+=" && echo \"$KEY\" >> ~/.ssh/authorized_keys"
    done
    ssh -4 $SUPER_PRIVATE_USER@$HOST -i ~/.ssh/id_super_private "$_CMD_"
    ssh -4 $SUPER_PRIVATE_USER@$HOST -i ~/.ssh/id_super_private "sudo mkdir -p /root/.ssh && sudo chmod go-rwx .ssh && sudo ssh-keygen -q -t ecdsa -f /root/.ssh/id_ecdsa$TEST -N '' "
    ssh -4 $SUPER_PRIVATE_USER@$HOST -i ~/.ssh/id_super_private "sudo cp /home/$SUPER_PRIVATE_USER/.ssh/authorized_keys /root/.ssh/"
    NEW_KEY=$(ssh $SUPER_PRIVATE_USER@$HOST -i ~/.ssh/id_super_private "sudo cat /root/.ssh/id_ecdsa$TEST.pub")
    echo "$NEW_KEY" >>~/.ssh/authorized_keys
    sudo sh -c "echo '$NEW_KEY' >> /root/.ssh/authorized_keys"
    echo "SSH_ROOT_KEYS+=( \"$NEW_KEY\" )" >>./oci-manage-config.sh
    SSH_ROOT_KEYS+=("$NEW_KEY")
    _ssh_super_secret_key_del
  fi
}

function init_deploy_admin_keys_to_admin_user() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_deploy_admin_keys_to_admin_user host username"
  else
    HOST="$1"
    USERNAME="$2"
    ssh -4 root@$HOST "cp /root/.ssh/authorized_keys /home/$USERNAME/.ssh/ && chown $USERNAME /home/$USERNAME/.ssh/authorized_keys"
  fi

}

function _ssh_super_secret_key() {
  echo -e $SSH_SUPER_PRIVATE_KEY >~/.ssh/id_super_private
  chmod go-rwx ~/.ssh/id_super_private
}

function _ssh_super_secret_key_del() {
  rm ~/.ssh/id_super_private
}

function init_create_admin_user_with_key() {
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_create_admin_user_with_key host username password"
  else
    HOST="$1"
    USERNAME="$2"
    PASSWORD="$3"
    _ssh_super_secret_key
    ssh -4 $SUPER_PRIVATE_USER@$HOST -i ~/.ssh/id_super_private "sudo useradd -m -s /bin/bash -p \"$PASSWORD\" \"$USERNAME\" && \
              sudo usermod -aG sudo \"$USERNAME\" && \
              sudo -u \"$USERNAME\" sh -c \"cd &&\
                                    mkdir -p .ssh &&\
                                    chmod go-rwx .ssh &&\
                                    ssh-keygen -q -t ecdsa -f ~/.ssh/id_ecdsa -N ''\""
    _ssh_super_secret_key_del
  fi
}

function init_set_ramdisk() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_set_ramdisk host"
  else
    HOST="$1"
    MEM_CMD="/usr/bin/grep MemTotal /proc/meminfo | /usr/bin/awk '{printf \"%d\",(\$2)*15/100000}'"
    REMOTE_MEM=$(ssh -4 root@$HOST "$MEM_CMD")
    REMOTE_MEM+="M"
    ssh -4 root@$HOST "mkdir -p /mnt/ramdisk && echo 'tmpfs       /mnt/ramdisk tmpfs   nodev,nosuid,noexec,nodiratime,size=$REMOTE_MEM   0 0' >> /etc/fstab"
  fi
}

function init_install_software() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_install_software host username"
  else
    HOST="$1"
    USERNAME="$2"
    ssh -4 root@$HOST "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg"
    ssh -4 root@$HOST 'curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | gpg --dearmor -o /etc/apt/trusted.gpg.d/google-k8s.gpg'
    ssh -4 root@$HOST 'curl -fsSL https://pkgs.k8s.io/core:/stable:/v'$KUBERNET_MAJOR_VERSION'/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg'
    ssh -4 root@$HOST 'echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/trusted.gpg.d/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list'
    #ssh -4 root@$HOST 'echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/trusted.gpg.d/google-k8s.gpg] http://apt.kubernetes.io/ kubernetes-xenial main"| tee /etc/apt/sources.list.d/k8s.list'
    ssh -4 root@$HOST 'echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v'$KUBERNET_MAJOR_VERSION'/deb/ /" | tee /etc/apt/sources.list.d/kubernetes.list'
    ssh -4 root@$HOST 'apt update && apt dist-upgrade -y && apt install -y iputils-ping psmisc cron vim wireguard iputils-ping docker-ce docker-ce-cli containerd.io docker-compose-plugin kubeadm kubelet kubectl kubernetes-cni haproxy jq nfs-common'
    ssh -4 root@$HOST "usermod -aG docker $USERNAME && reboot"
    cluster_minio_install_mc $HOST
  fi
}

function cluster_get_host_external_ipv4or6() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_get_host_external_ipv4 host AorAAAA"
  else
    HOST=$1
    QUERY=$2
    IP=$(dig +short $HOST.$DNS_PUBLIC_ZONE $QUERY)
    echo -e "$HOST\t$IP"
  fi
}

function cluster_get_external_ips() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_get_external_ips"
  else
    for MEMBER in $K8S_CLUSTER; do
      cluster_get_host_external_ipv4or6 $MEMBER A
      cluster_get_host_external_ipv4or6 $MEMBER AAAA
    done
  fi
}
function get_host_arch() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_host_arch host"
  else
    HOST="$1"
    ARCH=$(ssh -4 root@$HOST "dpkg --print-architecture")
    echo $ARCH
  fi
}

function init_build_cri_dockerd() {
  PWD=$(pwd)
  sudo apt-get -y install git golang-go
  git clone https://github.com/Mirantis/cri-dockerd.git
  cd cri-dockerd
  mkdir bin
  go get && go build -o bin/cri-dockerd
  mkdir -p /usr/local/bin
  sudo install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd
  sudo cp -a packaging/systemd/* /etc/systemd/system
  sudo sed -i -e 's,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,' /etc/systemd/system/cri-docker.service
  cd $PWD
  rm -rf cri-dockerd
}

function cluster_update_k8s_pause() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_update_k8s_pause version (ex 3.9)"
  else
    VERSION=$1
    for MEMBER in $K8S_CLUSTER; do
      echo $MEMBER
      ssh root@$MEMBER "sed -ibak 's/sandbox_image.*pause.*/sandbox_image = \"registry.k8s.io\/pause:$VERSION\"/' /etc/containerd/config.toml"
      ssh root@$MEMBER "systemctl restart containerd"
    done

  fi
}
function init_install_cri_containerd() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_install_cri_containerd host"
  else
    HOST="$1"
    ssh root@$HOST "containerd config default | sed 's/SystemdCgroup = false.*$/SystemdCgroup = true/g' | tee /etc/containerd/config.toml"
    #ssh root@$HOST "containerd config default | tee /etc/containerd/config.toml"
    #ssh root@$HOST "sed -ibak 's/^disabled_plugins = \[\"cri\"\]$/#disabled_plugins = \[\"cri\"\]/' /etc/containerd/config.toml"
    #ssh root@$HOST "grep -qxF '[plugins.\"io.containerd.grpc.v1.cri\"]' /etc/containerd/config.toml || echo -e '[plugins.\"io.containerd.grpc.v1.cri\"]\n  sandbox_image = \"registry.k8s.io/pause:3.9\"' >> /etc/containerd/config.toml "
    ssh root@$HOST "systemctl disable cri-docker ; systemctl disable cri-docker.socket ; systemctl enable containerd ; systemctl restart containerd"
  fi
}

function cluster_init_install_cri_containerd() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_cri_container"
  else
    for MEMBER in $K8S_CLUSTER; do
      echo $MEMBER
      init_install_cri_containerd $MEMBER
    done

  fi
}
function init_install_cri_docker() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_install_cri_docker host"
  else
    HOST="$1"
    ARCH=$(ssh -4 root@$HOST "dpkg --print-architecture")
    if [ "$ARCH" == "arm64" ]; then
      SRC_HOST=$ARM_SRC
    else
      SRC_HOST=$X86_64_SRC
    fi
    ssh root@$HOST "sed -i.bak '/^\[Service\].*/a MountFlags=shared' /lib/systemd/system/docker.service"
    scp -4 root@$SRC_HOST:/etc/systemd/system/cri-docker.service /tmp/
    scp -4 root@$SRC_HOST:/etc/systemd/system/cri-docker.socket /tmp/
    scp -4 root@$SRC_HOST:/usr/local/bin/cri-dockerd /tmp/
    scp -4 /tmp/cri-docker.service root@$HOST:/etc/systemd/system/cri-docker.service
    scp -4 /tmp/cri-docker.socket root@$HOST:/etc/systemd/system/cri-docker.socket
    scp -4 /tmp/cri-dockerd root@$HOST:/usr/local/bin/cri-dockerd
    ssh -4 root@$HOST "systemctl daemon-reload && systemctl enable cri-docker.service && systemctl enable --now cri-docker.socket"
    rm -f /tmp/cri-docker.service /tmp/cri-dockerd
  fi
}

function init_set_iptables() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_set_iptables host"
  else
    HOST="$1"
    scp -4 /etc/iptables/rules.v4 root@$HOST:/etc/iptables/rules.v4
    #ssh -4 root@$HOST "iptables-restore -t /etc/iptables/rules.v4"
    ssh -4 root@$HOST "systemctl restart iptables.service"
  fi
}

function init_set_iptables_members() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_set_iptables_members"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      PREFIX=${MEMBER:0:4}
      #if [ $PREFIX == "oci-" ]; then
      echo $MEMBER
      init_set_iptables $MEMBER
      #fi
    done
  fi
}
function init_install_fill_memory() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_install_fill_memory host"
  else
    HOST="$1"
    scp -4 ./fill-memory.sh root@$HOST:/etc/cron.hourly/fill-memory.sh
    ssh -4 root@$HOST "/etc/cron.hourly/fill-memory.sh"
  fi
}

function master_ssh_set_root_admin_keys() {
  sudo sh -c 'echo '' > /root/.ssh/authorized_keys'
  for KEY in "${SSH_ROOT_KEYS[@]}"; do
    _CMD_="echo \"$KEY\" >> /root/.ssh/authorized_keys"
    sudo sh -c "$_CMD_"
  done
}

function cluster_ssh_set_admin_keys() {
  sudo cp /root/.ssh/authorized_keys /tmp/authorized_keys
  sudo chmod ugo+rw /tmp/authorized_keys
  for MEMBER in $CLUSTER_MEMBERS; do
    echo $MEMBER
    scp /tmp/authorized_keys root@$MEMBER:/root/.ssh/authorized_keys
    ssh root@$MEMBER "chmod 0600 /root/.ssh/authorized_keys && cp /root/.ssh/authorized_keys /home/$USER/.ssh/authorized_keys && chown $USER /home/$USER/.ssh/authorized_keys"
  done
  sudo rm /tmp/authorized_keys
}

function cluster_delete_node() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_delete_node host"
  else
    NODE_ID=$1
    kubectl cordon $NODE_ID
    kubectl drain --delete-emptydir-data --force --ignore-daemonsets $NODE_ID
    kubectl delete node $NODE_ID
    cluster_reset_member $NODE_ID
  fi
}
function cluster_iptables_flush() {
  for MEMBER in $CLUSTER_MEMBERS; do
    echo $MEMBER
    ssh root@$MEMBER 'iptables -F && iptables -X'
  done
  sudo iptables -F && sudo iptables -X
}
function cluster_reset_storage() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_reset_storage"
  else
    for MEMBER in $K8S_CLUSTER; do
      echo $MEMBER
      ssh root@$MEMBER "rm -rf /storage/*"
    done
  fi
}

function cluster_reset_member() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_reset_member member"
  else
    MEMBER=$1
    ssh root@$MEMBER "systemctl stop docker; crictl --runtime-endpoint $CRI_SOCKET rm -a  ; kubeadm reset -f --cleanup-tmp-dir  --ignore-preflight-errors all --cri-socket=$CRI_SOCKET ; \
        rm -rf ~/.ku* ; \
        rm -rf /etc/systemd/system/kubelnet.service.d ; \
        rm -rf /var/lib/kubelet/ ; \
        rm -rf /var/lib/etcd ; \
        rm -rf /usr/libexec/kubernetes ; \
        rm -rf /etc/kubernetes ; \
        rm -rf /var/lib/weave/ ; \
        rm -rf /var/lib/cni ; \
        rm -rf /etc/cni ; \
        docker system prune -a -f ; \
        rm -rf /root/.kube ; \
        rm -rf /home/$USER/.kube ; \
        rm -f /etc/systemd/resolved.conf.d/k8s-resolve.conf ; \
        # reboot"
  fi
}
function cluster_reset_members() {
  for MEMBER in $CLUSTER_MEMBERS; do
    echo $MEMBER
    cluster_reset_member $MEMBER
  done
}

function cluster_reset_full() {
  cluster_reset_members
  # eventuellement une fois les membres disponibles
  cluster_reset_storage
  # control_plane
  cluster_reset_control_plane
}

function cluster_reset_control_plane() {
  sudo kubeadm reset -f --cri-socket=$CRI_SOCKET &&
    sudo rm -rf ~/.ku* &&
    sudo rm -rf /etc/systemd/system/kubelnet.service.d &&
    sudo rm -rf /var/lib/kubelet/ &&
    sudo rm -rf /var/lib/etcd &&
    sudo rm -rf /usr/libexec/kubernetes &&
    sudo rm -rf /etc/kubernetes &&
    sudo rm -rf /var/lib/weave/ &&
    sudo rm -rf /var/lib/cni &&
    sudo rm -rf /etc/cni &&
    docker system prune -a -f &&
    sudo rm -rf /root/.kube
  sudo rm -f /etc/systemd/resolved.conf.d/k8s-resolve.conf
  sudo systemctl restart systemd-resolved
  sudo rm -rf $HOME/.kube &&
    sudo reboot
}

function cluster_init_create_control_plane_with_kube_proxy() {
  if [ -z "${FUNCNAME[1]}" ]; then
    echo "call cluster_init_create_control_plane"
    IPAM="ipam.mode=cluster-pool,ipam.operator.clusterPoolIPv4PodCIDRList=$CILIUM_CLUSTER_POOL_IPV4_KUBERNETES_POD_CIDR"
  else
    echo "call cluster_init_create_control_plane_pool_kubernetes"
    IPAM="ipam.mode=kubernetes"
  fi
  sudo mkdir -p /etc/kubernetes/pki/etcd
  sudo mkdir -p /etc/kubernetes/manifests
  sudo cp -av /home/$USER/pki/* /etc/kubernetes/pki/
  sudo chown -R root:root /etc/kubernetes
  if [[ $DOCKER_RUNTIME == "containerd" ]]; then
    sudo systemctl disable cri-docker.service
    sudo systemctl disable cri-docker.socket
  else
    sudo systemctl daemon-reload
    sudo systemctl enable cri-docker.service
    sudo systemctl enable --now cri-docker.socket
  fi
  sudo kubeadm init \
    --ignore-preflight-errors=NumCPU \
    --pod-network-cidr=$KUBERNETES_POD_CIDR --service-cidr=$KUBERNETES_SERVICE_CIDR \
    --cri-socket=$CRI_SOCKET \
    --control-plane-endpoint=$CONTROL_PLANE_INTERNAL_ADDRESS \
    --node-name $HOSTNAME \
    --apiserver-advertise-address=$(/sbin/ip -o -4 addr list eth0 | awk '{print $4}' | cut -d/ -f1)
  set_systemd_resolve_to_k8s
  rm -rf $HOME/.kube
  mkdir -p $HOME/.kube
  sudo mkdir -p /root/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo cp -i /etc/kubernetes/admin.conf /root/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  if [ ! -z "$EXTERNAL_CLUSTER_ZONE" ]; then
    cluster_init_coredns_config
  fi
  #cilium install --version $CILIUM_VERSION --helm-set k8sServiceHost=$CONTROL_PLANE_IP,k8sServicePort=$CONTROL_PLANE_PORT,$IPAM,tunnel=vxlan,bpf.masquerade=true,bgpControlPlane.enabled=true,bgp.announce.loadbalancerIP=true,bgp.announce.podCIDR=true,hubble.relay.enabled=true,hubble.ui.enabled=true
  cluster_cilium_install
  sleep 60
  cluster_init_create_ip_pool
}

function cluster_cilium_install() {
  METHOD=install
  REPLACEMENT="disabled"
  if [ "$KUBE_PROXY_REPLACEMENT" == "true" ]; then
    REPLACEMENT="strict"
  fi
  if [ -z "$IPAM" ]; then
    IPAM="ipam.mode=cluster-pool,ipam.operator.clusterPoolIPv4PodCIDRList=$CILIUM_CLUSTER_POOL_IPV4_KUBERNETES_POD_CIDR,ipam.operator.clusterPoolIPv6PodCIDRList=$CILIUM_CLUSTER_POOL_IPV6_KUBERNETES_POD_CIDR,kubeProxyReplacement=$REPLACEMENT"
  fi
  CILIUM_CONFIG="--set envoy.enabled=$KUBE_PROXY_REPLACEMENT --set k8sServiceHost=$CONTROL_PLANE_IP \
      --set k8sServicePort=$CONTROL_PLANE_PORT \
      --set $IPAM --set tunnel=vxlan \
      --set ipv6.enabled=false \
      --set bpf.masquerade=true \
      --set bgpControlPlane.enabled=false \
      --set bgp.announce.loadbalancerIP=false \
      --set bgp.announce.podCIDR=false \
      --set hubble.relay.enabled=true \
      --set hubble.ui.enabled=true \
      --set prometheus.enabled=true \
      --set operator.prometheus.enabled=true \
      --set hubble.metrics.enabled=true \
      --set hubble.metrics.enableOpenMetrics=true \
      --version $CILIUM_VERSION "
  if [[ "$1" == "UNINSTALL" ]]; then
    helm uninstall cilium --namespace=kube-system
  else
    #IPAM="ipam.mode=cluster-pool,ipam.operator.clusterPoolIPv4PodCIDRList=$CILIUM_CLUSTER_POOL_IPV4_KUBERNETES_POD_CIDR"
    helm repo add cilium https://helm.cilium.io/
    echo $CILIUM_CONFIG
    helm upgrade --install --namespace=kube-system --create-namespace cilium cilium/cilium \
      $CILIUM_CONFIG
  #kubectl apply -f /tmp/cilium.yaml
  #rm -f /tmp/cilium.yaml
  fi

}

function cluster_init_create_control_plane() {
  REPLACEMENT=disabled
  if [[ "$KUBE_PROXY_REPLACEMENT" == "true" ]]; then
    REPLACEMENT=strict
  fi
  if [ -z "${FUNCNAME[1]}" ]; then
    echo "call cluster_init_create_control_plane"
    IPAM="ipam.mode=cluster-pool,ipam.operator.clusterPoolIPv4PodCIDRList=$CILIUM_CLUSTER_POOL_IPV4_KUBERNETES_POD_CIDR,ipam.operator.clusterPoolIPv6PodCIDRList=$CILIUM_CLUSTER_POOL_IPV6_KUBERNETES_POD_CIDR,kubeProxyReplacement=$REPLACEMENT"
  else
    echo "call cluster_init_create_control_plane_pool_kubernetes"
    IPAM="ipam.mode=kubernetes"
  fi
  sudo mkdir -p /etc/kubernetes/pki/etcd
  sudo mkdir -p /etc/kubernetes/manifests
  sudo cp -av /home/$USER/pki/* /etc/kubernetes/pki/
  sudo chown -R root:root /etc/kubernetes
  if [[ $DOCKER_RUNTIME == "containerd" ]]; then
    sudo systemctl disable cri-docker.service
    sudo systemctl disable cri-docker.socket
  else
    sudo systemctl daemon-reload
    sudo systemctl enable cri-docker.service
    sudo systemctl enable --now cri-docker.socket
  fi
  KUBEADM_CONFIG="--ignore-preflight-errors=NumCPU \
    --pod-network-cidr=$KUBERNETES_POD_CIDR --service-cidr=$KUBERNETES_SERVICE_CIDR \
    --cri-socket=$CRI_SOCKET \
    --control-plane-endpoint=$CONTROL_PLANE_INTERNAL_ADDRESS \
    --node-name $HOSTNAME \
    --apiserver-advertise-address=$(/sbin/ip -o -4 addr list eth0 | awk '{print $4}' | cut -d/ -f1)"

  if [[ "$KUBE_PROXY_REPLACEMENT" == true ]]; then
    KUBEADM_CONFIG+=" --skip-phases=addon/kube-proxy"
  fi
  echo "kubeadm init $KUBEADM_CONFIG"
  sudo kubeadm init $KUBEADM_CONFIG

  set_systemd_resolve_to_k8s
  rm -rf $HOME/.kube
  mkdir -p $HOME/.kube
  sudo mkdir -p /root/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo cp -i /etc/kubernetes/admin.conf /root/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  if [ -z "$EXTERNAL_CLUSTER_ZONE" ]; then
    cluster_init_coredns_config
  fi
  echo "Wait 1 minute for control plane being ready"
  sleep 60
  if [ ! -z "$EXTERNAL_CLUSTER_ZONE" ]; then
    cluster_init_coredns_config
  fi
  #kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/$GATEWAY_API_VERSION/experimental-install.yaml
  #cilium install --version $CILIUM_VERSION --helm-set kubeProxyReplacement=strict,k8sServiceHost=$CONTROL_PLANE_IP,k8sServicePort=$CONTROL_PLANE_PORT,$IPAM,tunnel=vxlan,bpf.masquerade=true,bgpControlPlane.enabled=true,bgp.announce.loadbalancerIP=true,bgp.announce.podCIDR=true,hubble.relay.enabled=true,hubble.ui.enabled=true,egressGateway.enabled=true,gatewayAPI.enabled=true
  cluster_cilium_install
  echo "sleeping for 60s"
  sleep 60
  cluster_init_create_ip_pool
}

function cluster_kubernetes_proxy_test() {
  cluster_run_on_all_as_current_user "curl --connect-timeout 2 https://172.24.0.1"
}

function cluster_init_create_control_plane_pool_kubernetes() {
  cluster_init_create_control_plane
}

function cluster_init_create_post_install() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_post_install"
  else
    cluster_setup_change_listener
    echo "sleeping 60s for waiting the change of listeners"
    sleep 60
    cluster_gateway_system_enable
    echo "sleeping 60s for waiting for gateway patch"
    sleep 60
    echo "now install haproxy"
    cluster_create_haproxy_ingress_class
    cluster_init_install_certmanager
    sleep 60
    #cluster_init_install_traefik
    cluster_init_cloudflare_dns_issuer
    cluster_init_azure_dns_issuer
    #cluster_init_install_oci_dns_issuer
    if [ "$STORAGE_BACKEND" == "longhorn" ]; then
      cluster_init_install_longhorn
      cluster_init_install_longhorn_ingress
    else
      cluster_init_install_openebs
    fi
    cluster_init_install_dashboard
    #cluster_init_install_traefik_ingressroute
    sleep 30
    #kubectl delete -n kube-traefik ingressroute.traefik.containo.us/traefik-dashboard
    #cluster_init_install_traefik_ingressroute
    #cilium hubble enable --ui
    cluster_init_cilium_hubble_ui_ingress
    cluster_run_on_all_members_as_root "systemctl restart systemd-resolved; systemctl stop haproxy && systemctl restart haproxy-ingress"
    if [[ $(type -t private_post_install) == function ]]; then
      private_post_install
    fi
    dev_install_local_registry
  fi
}

function cluster_init_create_another_control_plane() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_another_control_plane member"
  else
    MEMBER=$1
    MEMBER_IP=$(sed -n "s/^\(.*\) $MEMBER .*$/\1/p" /etc/hosts)

    ssh root@$MEMBER "mkdir -p /etc/kubernetes/pki/etcd"
    sudo scp -r /etc/kubernetes/pki/*ca*.* root@$MEMBER:/etc/kubernetes/pki/
    sudo scp -r /etc/kubernetes/pki/sa.* root@$MEMBER:/etc/kubernetes/pki/
    sudo scp -r /etc/kubernetes/pki/etcd/*ca*.* root@$MEMBER:/etc/kubernetes/pki/etcd/
    KUBEADM_EXTRA_ARGS="--control-plane --apiserver-advertise-address $MEMBER_IP --v=5" cluster_init_create_member $1
  fi
}

function cluster_delete_member() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_delete_member member"
  else
    MEMBER=$1
    MEMBER_IP=$(sed -n "s/^\(.*\) $MEMBER .*$/\1/p" /etc/hosts)
    kubectl drain $MEMBER --delete-emptydir-data --force --ignore-daemonsets
    kubectl delete node $MEMBER
    ssh root@$MEMBER "systemctl stop kubelet ; killall kube-scheduler ; killall kube-controller-manager ; killall kube-apiserver ; killall kube-proxy ; killall kubelet ; killall etcd ; killall csi-node-driver-registrar ; killall cilium-envoy ; killall cilium-agent ; killall cilium-operator ; killall cilium-health-responder ; killall cilium-etcd-operator ; killall cilium-etcd "
    ETCDID=$(cluster_etcd_get_member_list | sed -n "s/^\([a-f0-9]*\), .*, $MEMBER.*/\1/p")
    sudo ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member remove $ETCDID
  fi
}

function cluster_init_create_member() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_member member"
  else
    MEMBER=$1
    TOKEN=$(sudo kubeadm token create --print-join-command | awk '{print $5}')
    DISCOVERY_TOKEN_HASH=$(sudo kubeadm token create --print-join-command | awk '{print $7}')
    echo $MEMBER
    ssh root@$1 "echo 'SystemMaxUse=50M' >>/etc/systemd/journald.conf"
    if [[ $DOCKER_RUNTIME == "containerd" ]]; then
      ssh root@$MEMBER "systemctl disable cri-docker.service && systemctl disable cri-docker.socket"
    else
      ssh root@$MEMBER "systemctl daemon-reload && systemctl enable cri-docker.service && systemctl enable --now cri-docker.socket"
    fi
    ssh root@$MEMBER "mv /etc/resolv.conf /etc/resolv.conf.bak; ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf; mkdir -p /storage ; mkdir -p /etc/systemd/resolved.conf.d ; mkdir -p /root/.kube ; chmod go-rwx /root/.kube; rm -f /etc/kubernetes/*.conf ; rm -f /etc/kubernetes/manifests/* ; mkdir -p /etc/kubernetes/manifests; rm -rf /var/lib/etcd; kubeadm join $CONTROL_PLANE_INTERNAL_ADDRESS:6443 \
            --token $TOKEN \
            --discovery-token-ca-cert-hash $DISCOVERY_TOKEN_HASH \
            --cri-socket=$CRI_SOCKET $KUBEADM_EXTRA_ARGS"
    sudo scp /root/.kube/config root@$MEMBER:/root/.kube/config
    sudo scp /etc/systemd/resolved.conf.d/k8s-resolve.conf root@$MEMBER:/etc/systemd/resolved.conf.d/k8s-resolve.conf
    ssh root@$MEMBER "systemctl restart systemd-resolved && systemctl restart haproxy-ingress"
  fi
}
function cluster_gateway_system_enable() {
  kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/$GATEWAY_API_VERSION/experimental-install.yaml
}

function cluster_init_sysctl_value() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_sysctl_value MEMBER"
  else
    MEMBER=$1
    member_enable_sysctl_value $MEMBER net.ipv4.ip_forward
    member_enable_sysctl_value $MEMBER net.ipv6.conf.all.forwarding
    member_enable_sysctl_value $MEMBER net.bridge.bridge-nf-call-iptables
    member_enable_sysctl_value $MEMBER net.bridge.bridge-nf-call-ip6tables
  fi
}

function cluster_init_create_members() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_members"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      cluster_init_sysctl_value $MEMBER
      cluster_init_create_member $MEMBER
    done
  fi
}

function cluster_init_install_traefik_ingressroute() {
  if [ "$#" -ne 1 ]; then
    ACTION=apply
  else
    ACTION=$1
  fi
  HTTPASSWD=$(printf "$MASTER_ADMIN:$(openssl passwd -apr1 "$MASTER_ADMIN_PASSWORD")\n")
  HTTPASSWD_B64=$(echo -n "$HTTPASSWD" | base64)
  RES=""
  for NAME in "${TRAEFIK_DASHBOARD_DNS_NAMES[@]}"; do RES+="Host(\`$NAME\`) || "; done
  RULES="${RES:0:-3}"
  MATCH="(((PathPrefix(\`/api\`) || PathPrefix(\`/dashboard\`))) && ($RULES) )"
  CN=${TRAEFIK_DASHBOARD_DNS_NAMES[0]}
  cat <<EOF | kubectl $ACTION -f -
apiVersion: v1
kind: Secret
metadata:
  name: authsecret
  namespace: kube-traefik
data:
  users: $HTTPASSWD_B64
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: traefik-dashboard-certs
  namespace: kube-traefik
  labels:
     k8s-app: kube-traefik
spec:
  subject:
      organizations: 
        - $CA_COMPANY
  commonName: $CN
  dnsNames: $TRAEFIK_DASHBOARD_DNS_NAMES_JS
  secretName: traefik-dashboard-certs
  issuerRef:
    name: $CA_COMPANY-ca-issuer
    kind: ClusterIssuer
---
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  name: traefik-dashboard-auth
  namespace: kube-traefik
spec:
  basicAuth:
    secret: authsecret
---
apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: traefik-dashboard
  namespace: kube-traefik
spec:
  entryPoints:
    - websecure
  routes:
    - match: $MATCH
      kind: Rule
      services:
      - name: api@internal
        kind: TraefikService
      middlewares:
      - name: traefik-dashboard-auth
  tls:
    secretName: traefik-dashboard-certs
---
EOF
}
function cluster_init_install_traefik() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_traefik"
  else
    helm repo add traefik https://traefik.github.io/charts
    helm repo update
    helm upgrade --install --create-namespace --namespace=kube-traefik --values ./miscellaneous/haproxy-traefik-websecure.yaml \
      traefik traefik/traefik
  fi
}

function cluster_init_install_certmanager() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_certmanager"
  else
    helm repo add jetstack https://charts.jetstack.io
    helm repo update
    # kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.3/cert-manager.crds.yaml
    helm install --create-namespace --namespace=kube-certmanager \
      cert-manager jetstack/cert-manager \
      --set installCRDs=true
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: "$(date --iso-8601=seconds --utc)"
  labels:
    kubernetes.io/metadata.name: kube-certmanager
  name: kube-certmanager
---
apiVersion: v1
kind: Secret
metadata:
  name: ca-key-pair
  namespace: kube-certmanager
data:
    # cat ca.test.com.cer | base64 | tr -d '\n'
  tls.crt: $CA
  tls.key: $CA_KEY
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: $CA_COMPANY-ca-issuer
spec:
  ca:
    secretName: ca-key-pair
EOF
  fi
}

function cluster_init_install_dashboard() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_dashboard"
  else
    CN=${DASHBOARD_DNS_NAMES[0]}
    # create namespace
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: kubernetes-dashboard
EOF
    cat <<EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: kubernetes-dashboard-certs
  namespace: kubernetes-dashboard
  labels:
      k8s-app: kubernetes-dashboard
spec:
  subject:
      organizations:
        - $CA_COMPANY
  commonName: $CN
  dnsNames: $DASHBOARD_DNS_NAMES_JS
  secretName: kubernetes-dashboard-certs
  issuerRef:
    name: $CA_COMPANY-ca-issuer
    kind: ClusterIssuer
EOF
    kubectl apply -f ./dashboard/00-dashboard.yaml -f ./dashboard/01-admin-user.yaml #-f ./dashboard/02-ingress.yaml
    cluster_init_install_dashboard_ingressroute
  fi

}

function cluster_init_install_dashboard_ingressroute() {
  if [ "$#" -ne 1 ]; then
    ACTION=apply
  else
    ACTION=$1
  fi
  cat <<EOF >/tmp/cluster_init_install_dashboard_ingressroute.yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: dashboard-k8s
  namespace: kubernetes-dashboard
  annotations:
    cert-manager.io/cluster-issuer: $CA_COMPANY-ca-issuer
    haproxy.org/server-ssl: "true"
spec:
  ingressClassName: haproxy
  rules:
EOF
  for HOST in "${DASHBOARD_DNS_NAMES[@]}"; do
    cat <<EOF >>/tmp/cluster_init_install_dashboard_ingressroute.yaml
    - host: $HOST
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: kubernetes-dashboard
                port:
                  number: 443
EOF
  done
  cat <<EOF >>/tmp/cluster_init_install_dashboard_ingressroute.yaml
  tls:
  - hosts: $DASHBOARD_DNS_NAMES_JS
    secretName: dashboard-cert
---
EOF
  kubectl $ACTION -f /tmp/cluster_init_install_dashboard_ingressroute.yaml
  rm /tmp/cluster_init_install_dashboard_ingressroute.yaml
}

function _get_pool_yaml() {
  cat <<EOF >/dev/stdout
apiVersion: "cilium.io/v2alpha1"
kind: CiliumLoadBalancerIPPool
metadata:
  name: "oci-pool"
spec:
  cidrs:
  - cidr: $CILIUM_LB_IP_POOL
EOF
}

function cluster_init_create_ip_pool() {
  _get_pool_yaml | kubectl apply -f -
}

function cluster_delete_ip_pool() {
  _get_pool_yaml | kubectl delete -f -
}

function cluster_init_install_cri_docker() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_cri_docker member"
  else
    MEMBER=$1
    sudo scp /etc/systemd/system/cri-docker.* root@$MEMBER:/etc/systemd/system/
    sudo scp /usr/local/bin/cri-dockerd root@$MEMBER:/usr/local/bin/
    ssh root@$MEMBER 'systemctl daemon-reload'
    ssh root@$MEMBER 'systemctl enable cri-docker.service'
    ssh root@$MEMBER 'systemctl enable --now cri-docker.socket'
  fi
}

function get_subnet() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_subnet host"
  else
    NEW_MEMBER=$1
    echo "$(dig $NEW_MEMBER +short | head -n 1 | sed 's/\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\)/\1\2\3/')0/24"
  fi
}

function get_subnet_16() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_subnet host"
  else
    NEW_MEMBER=$1
    echo "$(dig $NEW_MEMBER +short | head -n 1 | sed 's/\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\)/\1\2/')0.0/16"
  fi
}

function get_subnet_from_ip() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_subnet_from_ip ip"
  else
    IP=$1
    echo "$(echo "$IP" | sed 's/\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\)/\1\2\3/')0/24"
  fi
}

function get_gateway() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_gateway host"
  else
    NEW_MEMBER=$1
    echo "$(dig $NEW_MEMBER +short | sed 's/\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\)/\1\2\3/')1"
  fi
}

function get_gateway_from_ip() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_gateway_from_ip ip"
  else
    IP=$1
    echo "$(echo "$IP" | sed 's/\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\)/\1\2\3/')1"
  fi
}

function init_create_private_interface() {
  PREFIX=${1:0:4}
  if [ "$#" -ne 4 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_create_private_interface shorthstname publicip macaddress privateipaddress" && echo "    Use it only for OCIs hosts"
  else
    if [ "$PREFIX" == "oci-" ]; then
      HOST=$1
      PUBLICIP=$2
      MAC=$3
      IP=$4
      GW=$(get_gateway_from_ip "$IP")
      SUBNET=$(get_subnet_from_ip "$IP")
      INTERFACE=$(get_remote_interface $PUBLICIP "$MAC")
      ROUTES_JSON=$(init_get_private_routes "$IP" | tr -d '\n')
      TEMP_FILE="/tmp/$HOST.yaml"
      echo "TEMP_FILE=$TEMP_FILE"
      echo "INTERFACE=$INTERFACE"
      echo "MAC=$MAC"
      echo "IP=$IP"
      echo "ROUTES_JSON=$ROUTES_JSON"
      cat <<EOF | yq -p json -o yaml - >"$TEMP_FILE"
{
  "network": {
    "version": 2,
    "ethernets": {
      "$INTERFACE": {
        "match": {
          "macaddress": "$MAC"
        },
        "addresses": ["$IP/24"],
        "set-name": "$INTERFACE",
        "routes": $ROUTES_JSON
      }
    }
  }
}
EOF
      echo "PROPOSAL:"
      cat "$TEMP_FILE"
      echo "Verify and if OK issue:"
      echo "scp -4 /tmp/$HOST.yaml root@$PUBLICIP:/etc/netplan/$PRIVATE_INTERFACE_FILE"
      echo "ssh -4 root@$PUBLICIP 'netplan apply'"
      echo "rm $TEMP_FILE"
    else
      echo "Illegal host:" && echo "    Use it only for OCIs hosts"
    fi
  fi
}
function get_remote_interface() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_remote_interface host macaddress"
  else
    HOST=$1
    MAC=$(echo $2 | awk '{print tolower($0)}')
    AWK_CMD="'\$3 ~ /$MAC/ {print \$1}'"
    SSH_CMD="ip -br link | awk $AWK_CMD"
    INTERFACE=$(ssh $USER@$HOST "ip -br link | awk $AWK_CMD")
    echo $INTERFACE
  fi
}

function get_remote_interface_from_ip() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_remote_interface_from_ip host"
  else
    HOST=$1
    IP=$(dig +short $HOST)
    AWK_CMD="'\$3 ~ /$IP/ {print \$1}'"
    SSH_CMD="ip -br link | awk $AWK_CMD"
    INTERFACE=$(ssh $USER@$HOST "ip -br add | awk $AWK_CMD")
    echo $INTERFACE
  fi
}

function get_remote_interface_mac() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_remote_interface_from_ip host device"
  else
    HOST=$1
    DEVICE=$2
    AWK_CMD="'\$1 ~ /$DEVICE/ {print \$3}'"
    SSH_CMD="ip -br link | awk $AWK_CMD"
    INTERFACE=$(ssh $USER@$HOST "ip -br link | awk $AWK_CMD")
    echo $INTERFACE
  fi
}

function cluster_recreate_master_private_interface() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_recreate_master_private_interface"
  else
    MEMBER=$CONTROL_PLANE_LOCAL
    MEMBER_IP=$CONTROL_PLANE_IP
    MEMBER_GW=$(get_gateway_from_ip $CONTROL_PLANE_IP)
    MEMBER_INTERFACE=$(ip -br add | awk "\$3 ~ /$MEMBER_IP/ {print \$1}")
    MEMBER_MAC=$(ip -br link | awk "\$1 ~ /$MEMBER_INTERFACE/ {print \$3}")
    ROUTES_JSON=$(init_get_private_routes $MEMBER_IP)
    echo "For $CONTROL_PLANE_LOCAL / $MEMBER_INTERFACE / $MEMBER_IP / $MEMBER_MAC / $MEMBER_GW "
    ######
    cat <<EOF | yq -p json -o yaml - >/tmp/$MEMBER.yaml
{
  "network": {
    "version": 2,
    "ethernets": {
      "$MEMBER_INTERFACE": {
        "match": {
          "macaddress": "$MEMBER_MAC"
        },
        "addresses": ["$MEMBER_IP/24"],
        "set-name": "$MEMBER_INTERFACE",
        "routes": $ROUTES_JSON
      }
    }
  }
}
EOF
    sudo cp /tmp/$MEMBER.yaml /etc/netplan/$PRIVATE_INTERFACE_FILE
    sudo netplan apply
  fi
}

function cluster_recreate_private_interface() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_recreate_private_interface"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      if [[ "$ROUTING_MEMBER" != "$MEMBER" ]]; then
        HAS_PRIVATE_INTERFACE=$(ssh root@$MEMBER "test -f /etc/netplan/$PRIVATE_INTERFACE_FILE && echo $?")
        if [[ "$HAS_PRIVATE_INTERFACE" -ge "0" ]] && [[ ! -z "$HAS_PRIVATE_INTERFACE" ]]; then
          if [[ "$MEMBER" != "$NEW_MEMBER" ]]; then
            MEMBER_IP=$(dig $MEMBER +short)
            MEMBER_GW=$(get_gateway $MEMBER)
            AWK_CMD_1="'/$MEMBER_IP/ {print \$1}'"
            MEMBER_INTERFACE=$(get_remote_interface_from_ip $MEMBER)
            MEMBER_MAC=$(get_remote_interface_mac $MEMBER $MEMBER_INTERFACE)
            ROUTES_JSON=$(init_get_private_routes $MEMBER_IP)
            echo "For $MEMBER / $MEMBER_INTERFACE / $MEMBER_IP / $MEMBER_MAC / $MEMBER_GW "
            cat <<EOF | yq -p json -o yaml - >/tmp/$MEMBER.yaml
{
  "network": {
    "version": 2,
    "ethernets": {
      "$MEMBER_INTERFACE": {
        "match": {
          "macaddress": "$MEMBER_MAC"
        },
        "addresses": ["$MEMBER_IP/24"],
        "set-name": "$MEMBER_INTERFACE",
        "routes": $ROUTES_JSON
      }
    }
  }
}
EOF
            scp -4 /tmp/$MEMBER.yaml root@$MEMBER:/etc/netplan/$PRIVATE_INTERFACE_FILE
            ssh -4 root@$MEMBER "reboot"
          fi
        else
          echo "$MEMBER doesn't have private interface (HAS_PRIVATE_INTERFACE=$HAS_PRIVATE_INTERFACE)"
        fi
      else
        echo "ATTENTION $MEMBER is the routing plane so it is excluded"
      fi
    done
    cluster_recreate_master_private_interface
  fi
}

function init_get_private_routes() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage init_get_private_routes ipaddress"
  else
    IP=$1
    rm -f /tmp/$IP.tmp
    NEW_MEMBER_SUBNET="$(get_subnet_from_ip $IP)"
    # echo "{\"to\":\"default\", \"via\": \"$MEMBER_GW\"}" >>/tmp/$IP.tmp
    for DEST_MEMBER in $K8S_CLUSTER_LONG; do
      DEST_MEMBER_SUBNET="$(get_subnet $DEST_MEMBER)"
      if [[ "$DEST_MEMBER" != "$IP" ]]; then
        if [[ "$NEW_MEMBER_SUBNET" != "$DEST_MEMBER_SUBNET" ]]; then
          MEMBER_GW="$(get_gateway_from_ip $IP)"
          echo "{\"to\":\"$DEST_MEMBER_SUBNET\", \"via\": \"$MEMBER_GW\"}" >>/tmp/$IP.tmp
        fi
      fi
      for ADDITIONAL_ROUTE in "${ADDITIONAL_ROUTES[@]}"; do
        echo "{\"to\":\"$ADDITIONAL_ROUTE\", \"via\": \"$MEMBER_GW\"}" >>/tmp/$IP.tmp
      done
    done
    awk -i inplace '!seen[$0]++' /tmp/$IP.tmp
    ROUTES=""
    while IFS= read -r line; do
      ROUTES+="$line,"
    done <"/tmp/$IP.tmp"
    rm -f /tmp/$IP.tmp
    ROUTES=${ROUTES::-1}
    echo "[$ROUTES]"
  fi
}
function v0_cluster_add_route_for_new_member() {
  echo "BUUUUGGGGGGED"
  exit 1
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_add_route_for_new_member member"
  else
    NEW_MEMBER=$1
    NEW_MEMBER_IP=$(dig $NEW_MEMBER +short)
    NEW_MEMBER_SUBNET="$(get_subnet $NEW_MEMBER)"
    echo "NEW_MEMBER=$NEW_MEMBER"
    echo "NEW_MEMBER_IP=$NEW_MEMBER_IP"
    echo "NEW_MEMBER_SUBNET=$NEW_MEMBER_SUBNET"
    for MEMBER in $CLUSTER_MEMBERS; do
      if [[ "$MEMBER" != "$NEW_MEMBER" ]]; then
        MEMBER_IP=$(dig $MEMBER +short)
        MEMBER_GW=$(get_gateway $MEMBER)
        echo "For $MEMBER"
        echo "netplan set ethernets.enp1s0.routes='[{"to":"$NEW_MEMBER_SUBNET", "via": "$MEMBER_GW"}]'"
      fi
    done
  fi
}

function cluster_create_private_cross_routes() {
  echo "BUUUUGGGGGGED"
  exit 1
  for MEMBER in $CLUSTER_MEMBERS; do
    rm -f /tmp/$MEMBER.tmp
    echo "For $MEMBER"
    MEMBER_SUBNET="$(get_subnet $MEMBER)"
    for DEST_MEMBER in $CLUSTER_MEMBERS; do
      DEST_MEMBER_SUBNET="$(get_subnet $DEST_MEMBER)"
      if [[ "$DEST_MEMBER" != "$MEMBER" ]]; then
        if [[ "$MEMBER_SUBNET" != "$DEST_MEMBER_SUBNET" ]]; then
          MEMBER_GW="$(get_gateway $MEMBER)"
          echo "{\"to\":\"$DEST_MEMBER_SUBNET\", \"via\": \"$MEMBER_GW\"}" >>/tmp/$MEMBER.tmp
        fi
      fi
    done
    awk -i inplace '!seen[$0]++' /tmp/$MEMBER.tmp
    ROUTES=""
    while IFS= read -r line; do
      ROUTES+="$line,"
    done <"/tmp/$MEMBER.tmp"
    rm -f /tmp/$MEMBER.tmp
    ROUTES=${ROUTES::-1}
    COMMAND="netplan set ethernets.enp1s0.routes='[$ROUTES]'"
    echo $ROUTES
    #ssh root@$MEMBER "netplan set ethernets.enp1s0.routes='[]' ; $COMMAND"
  done
}

function cluster_cilium_full_status() {
  CILIUM_PODS=$(kubectl get pods -A | grep cilium | awk '{print $2}' | xargs echo)
  for POD in $CILIUM_PODS; do
    IS_CILIUM_OPERATOR=$(echo "$POD" | grep 'operator\|envoy')
    if [[ $? -eq 1 ]]; then
      echo "##################"
      echo "##     $POD"
      echo "##################"
      kubectl -n kube-system exec $POD -- cilium status
    fi
  done
}

function cluster_cilium_hostip_all() {
  for MEMBER in $CLUSTER_MEMBERS; do
    echo $MEMBER
    ssh $USER@$MEMBER "ip add | grep -A3 cilium | grep 'inet '"
  done
  echo "Control plane"
  ip add | grep -A3 cilium | grep 'inet '
}

function cluster_test_udp_port() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_test_udp_port port"
  else
    PORT=$1
    echo $PORT
    netcat -4 -k -l -u $PORT &
    PID=$!

    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      COMMAND='echo "hello from `hostname`" | '
      COMMAND+="netcat -4 -q 1 $CONTROL_PLANE_INTERNAL_ADDRESS -u $PORT"
      ssh $USER@$MEMBER "$COMMAND"
    done
    kill -9 $PID
  fi
}
function cluster_test_tcp_port() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_test_tcp_port port"
  else
    PORT=$1
    echo $PORT
    netcat -4 -k -l $PORT &
    PID=$!

    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      COMMAND='echo "hello from `hostname`" | '
      COMMAND+="netcat -4 -q 1 $CONTROL_PLANE_INTERNAL_ADDRESS $PORT"
      ssh $USER@$MEMBER "$COMMAND"
    done
    kill -9 $PID
  fi
}

function cluster_cilium_hostip_all_as_for() {
  cluster_cilium_hostip_all | grep inet | awk -v RS='([0-9]+\\.){3}[0-9]+' 'RT{print RT}' | xargs echo
}

function cluster_cilium_hostip_cross_ping() {
  CILIUM_HOSTS=$(cluster_cilium_hostip_all_as_for)
  for MEMBER in $CLUSTER_MEMBERS; do
    echo "------------------------------------------------------------------------------------------------------------------------------------------"
    echo $MEMBER
    COMMAND="CILIUM_HOSTS='$CILIUM_HOSTS' ; "
    COMMAND+='for IP in $CILIUM_HOSTS;'
    COMMAND+='do ping -c1 -W1 $IP;done '
    ssh $USER@$MEMBER "$COMMAND"
  done
}

function cluster_ping_cross_from_members() {
  for MEMBER in $CLUSTER_MEMBERS; do
    echo "------------------------------------------------------------------------------------------------------------------------------------------"
    echo $MEMBER
    COMMAND="HOSTS='$K8S_CLUSTER' ; "
    COMMAND+='for IP in $HOSTS;'
    COMMAND+='do ping -c1 -W1 $IP | head -n2; done '
    ssh $USER@$MEMBER "$COMMAND"
  done
}

function cluster_reboot() {
  for MEMBER in $CLUSTER_MEMBERS; do
    echo "$MEMBER"
    ssh root@$MEMBER 'reboot --no-wall'
  done
  sudo reboot
}

function cluster_apt_dist_upgrade() {
  cluster_run_on_all_as_root "apt update -y 1> /dev/null 2> /dev/null && NEEDRESTART_MODE=a apt dist-upgrade -y && apt autoremove -y"
  # for MEMBER in $CLUSTER_MEMBERS; do
  #   echo $MEMBER
  #   ssh root@$MEMBER 'apt update -y && NEEDRESTART_MODE=a apt dist-upgrade -y && NEEDRESTART_MODE=a apt upgrade -y && apt autoremove -y'
  # done
  # sudo apt update -y 1> /dev/null 2> /dev/null && sudo apt dist-upgrade -y && sudo apt upgrade -y && sudo apt autoremove -y
}

function cluster_apt_list_upgradable() {
  cluster_run_on_all_as_root "apt update -y 1> /dev/null 2> /dev/null && apt list --upgradable"
}

function cluster_needrestart() {
  for MEMBER in $CLUSTER_MEMBERS; do
    echo $MEMBER
    ssh $MEMBER 'needrestart'
  done
  needrestart
}
function cluster_apt_install() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_apt_install"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      ssh root@$MEMBER "apt install -y $1"
    done
  fi
  sudo apt install -y $1
}

function cluster_run_on_all_members_as_root() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_run_on_all_members_as_root command"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      ssh root@$MEMBER $1
    done
  fi
}

function cluster_run_on_all_as_root() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_run_on_all_members_as_root command"
  else
    for MEMBER in $K8S_CLUSTER; do
      echo $MEMBER
      ssh root@$MEMBER $1
    done
  fi
}

function cluster_run_on_all_members_as_current_user() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_run_on_all_members_as_current_user command"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      ssh $USER@$MEMBER $1
    done
  fi
}

function cluster_run_on_all_as_current_user() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_run_on_all_members_as_current_user command"
  else
    for MEMBER in $K8S_CLUSTER; do
      echo $MEMBER
      ssh $USER@$MEMBER $1
    done
  fi
}

function cluster_copy_file_as_current_user() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_copy_file_as_current_user local_file remote_file"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      scp $1 $USER@$MEMBER:$2
    done
  fi
}

function cluster_deploy_hosts() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_deploy_hosts"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      #if [ ${MEMBER:0:4} == "oci-" ]; then    #for oci only
      sed -e '/^127\..*/d' -e '/^[[:space:]]*$/d' /etc/hosts >/tmp/$MEMBER
      echo "127.0.1.1    $MEMBER     $MEMBER" >>/tmp/$MEMBER
      scp /tmp/$MEMBER root@$MEMBER:/etc/hosts
      rm /tmp/$MEMBER
      #fi
    done
  fi
}

function cluster_copy_file_and_run_as_root() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_copy_file_as_root local_file remote_file"
  else
    LOCAL_FILE=$1
    REMOTE_FILE=$2
    cluster_copy_file_as_root $LOCAL_FILE $REMOTE_FILE
    cluster_run_on_all_members_as_root $REMOTE_FILE
  fi
}

function cluster_copy_file_as_root() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_copy_file_as_root local_file remote_file"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      scp $1 root@$MEMBER:$2
    done
  fi
}

function cluster_copy_dir_as_root() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_copy_dir_as_root local_dir remote_dir"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      echo $MEMBER
      scp -r $1 root@$MEMBER:$2
    done
  fi
}

function dashboard_get_token() {
  kubectl -n kubernetes-dashboard create token admin-user
}

function wg_meshconf_init() {
  PWD=$(pwd)
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage wg_meshconf_init"
  else
    if [ ! -f ~/wg/database.csv ]; then
      echo "init wg meshconf!"
      mkdir -p ~/wg
      docker run -v $WG_DATABASE:/home/wgmeshconf -it highcanfly/wg-meshconf init
    fi
  fi
  cd $PWD
}

function wg_meshconf_showpeers() {
  PWD=$(pwd)
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage wg_meshconf_showeers"
  else
    cd ~
    wg_meshconf_init
    docker run -v $WG_DATABASE:/home/wgmeshconf -it highcanfly/wg-meshconf showpeers
  fi
  cd $PWD
}

function wg_meshconf_get_all_peers_sorted() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage wg_meshconf_get_all_peers_sorted"
  else
    cat wg/database.csv | awk -F "," '{ print $2}' | tail -n +2 | sed -e 's/^"//' -e 's/"$//' | grep -o '[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}' | sort -t . -k 3,3n -k 4,4n
  fi
}
function wg_meshconf_get_all_peers_sorted_except_gw() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage wg_meshconf_get_all_pewg_meshconf_get_all_peers_sorted_except_gwers_sorted"
  else
    wg_meshconf_get_all_peers_sorted | sort -t . -k 3,3n -k 4,4n | tail -n 2 | head -n 1
  fi
}
function wg_meshconf_addpeer() {
  PWD=$(pwd)
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage wg_meshconf_addpeer peername peer_fqdn peer_udp_port"
  else
    PEER_NAME=$1
    PEER_FQDN=$2
    PEER_UDP_PORT=$3
    cd ~
    wg_meshconf_init
    ALL_PEERS_SORTED=$(wg_meshconf_get_all_peers_sorted)
    GREATEST_PEER_EXCEPT_GW=$(wg_meshconf_get_all_peers_sorted_except_gw)
    NEW_PEER=$(echo $GREATEST_PEER_EXCEPT_GW | awk -F "." '{printf "%d.%d.%d.%d",$1,$2,$3,($4+1)}')
    SUBNET=$(get_subnet_16 $PEER_NAME)
    echo "Adding $NEW_PEER/32"
    docker run -v $WG_DATABASE:/home/wgmeshconf -it highcanfly/wg-meshconf addpeer $PEER_NAME --fwmark "$WG_MARK" --table $WG_TABLE --address $NEW_PEER/32 --endpoint $PEER_FQDN --listenport $PEER_UDP_PORT --allowedips $SUBNET
  fi
  cd $PWD
}

function wg_meshconf_get_fqdns() {
  awk -F "\"*,\"*" '{print $3}' wg/database.csv | tail -n +2
}

function wg_meshconf_deploy_config() {
  PWD=$(pwd)
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage wg_meshconf_deploy_config"
  else
    cd ~
    wg_meshconf_init
    docker run -v ./wg:/home/wgmeshconf -it highcanfly/wg-meshconf genconfig
    #echo "$WG_TABLE     wireguard" >/tmp/wireguard.conf
    #sudo cp /tmp/wireguard.conf /etc/iproute2/rt_tables.d/wireguard.conf
    #rm /tmp/wireguard.conf
    sudo cp ~/miscellaneous/wg-quick-oci /usr/bin/
    sudo cp ~/miscellaneous/wg-quick-oci.target /lib/systemd/system/
    sudo cp ~/miscellaneous/wg-quick-oci@.service /lib/systemd/system/
    sudo systemctl daemon-reload
    for MEMBER in $(wg_meshconf_get_fqdns); do
      MEMBER_SHORT=$(echo -n $MEMBER | cut -d"." -f1)
      if [ -f "wg/output/$MEMBER_SHORT.conf" ]; then
        echo $MEMBER
        scp -4 wg/output/$MEMBER_SHORT.conf root@$MEMBER:/etc/wireguard/wg0.conf
        #scp -4 /etc/iproute2/rt_tables.d/wireguard.conf root@$MEMBER:/etc/iproute2/rt_tables.d/wireguard.conf
        scp -4 ~/miscellaneous/wg-quick-oci root@$MEMBER:/usr/bin/wg-quick-oci
        scp -4 ~/miscellaneous/wg-quick-oci.target root@$MEMBER:/lib/systemd/system/wg-quick-oci.target
        scp -4 ~/miscellaneous/wg-quick-oci@.service root@$MEMBER:/lib/systemd/system/wg-quick-oci@.service
        ssh -4 root@$MEMBER "systemctl daemon-reload; systemctl enable wg-quick-oci@wg0 ; systemctl restart wg-quick-oci@wg0"
      fi
    done
    sudo cp wg/output/$CONTROL_PLANE_LOCAL.conf /etc/wireguard/wg0.conf
    sudo systemctl enable wg-quick-oci@wg0
    sudo systemctl restart wg-quick-oci@wg0
    #### private
    if [[ $(type -t private_wg_deploy) == function ]]; then
      private_wg_deploy
    fi
  fi
  cd $PWD
}

function oci_get_data() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_get_data key (exemple mytenancy.vcn.privaye.cidr)"
  else
    COMKEY=$1
    KEY=$(echo "$COMKEY" | cut -d. -f2-)
    TENANCY=$(echo "$COMKEY" | cut -d. -f1)
    _CMD_="'.[] | select(.tenancy == \"$TENANCY\")'.$KEY"
    sh -c "yq $_CMD_ < $OCI_DB"
  fi
}

function cluster_ping_host_from_members() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_ping_host_from_members host"
  else
    HOST=$1
    for MEMBER in $CLUSTER_MEMBERS; do
      echo "From $MEMBER to $HOST"
      ssh $USER@$MEMBER "ping -c1 $HOST"
    done
    echo "From localhost to $HOST"
    ping -c1 $HOST
  fi
}

function cluster_ping6_host_from_members() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_ping6_host_from_members host"
  else
    HOST=$1
    for MEMBER in $CLUSTER_MEMBERS; do
      echo "From $MEMBER to $HOST"
      ssh $USER@$MEMBER "ping6 -c1 $HOST"
    done
    echo "From localhost to $HOST"
    ping6 -c1 $HOST
  fi
}

cluster_get_all_public_ip() {
  for MEMBER in $K8S_CLUSTER; do
    IPv4=$(dig +short $MEMBER.$CLUSTER_DOMAIN)
    IPv6=$(dig +short $MEMBER.$CLUSTER_DOMAIN AAAA)
    echo -e "$MEMBER\t$IPv4\t$IPv6"
  done
}
function cluster_get_traefik_lb_ip() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_get_traefik_lb_ip"
  else
    #LB_IP=$(kubectl get services -A | awk '$2=="traefik" && $3=="LoadBalancer" {print $5}')
    LB_IP=$(kubectl get services \
      --namespace kube-traefik \
      traefik \
      --output jsonpath='{.status.loadBalancer.ingress[0].ip}')
    echo $LB_IP
  fi
}

function cluster_traefik_launch_simple_proxy() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_get_traefik_lb_ip"
  else
    sudo simpleproxy -v -L $CONTROL_PLANE_IP:443 -R $(cluster_get_traefik_lb_ip):443
  fi
}

function cluster_init_cilium_hubble_ui_ingress() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_cilium_hubble_ui_ingress"
  else
    cat <<EOF >/tmp/cluster_init_cilium_hubble_ui_ingress.yaml
apiVersion: v1
kind: Secret
metadata:
  namespace: kube-system
  name: hubble-ui-haproxy-credentials
data:
  $MASTER_ADMIN: $MASTER_ADMIN_PASSWORD_CRYPTB64
type: Opaque
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hubble-ui
  namespace: kube-system
  annotations:
    cert-manager.io/cluster-issuer: $CA_COMPANY-ca-issuer
    haproxy.org/auth-type: basic-auth
    haproxy.org/auth-secret: kube-system/hubble-ui-haproxy-credentials
    # traefik.ingress.kubernetes.io/router.entrypoints: websecure
    # traefik.ingress.kubernetes.io/router.middlewares: kube-traefik-traefik-dashboard-auth@kubernetescrd
spec:
  ingressClassName: haproxy
  rules:
EOF
    for HOST in "${HUBBLE_DASHBOARD_DNS_NAMES[@]}"; do
      cat <<EOF >>/tmp/cluster_init_cilium_hubble_ui_ingress.yaml
    - host: $HOST
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: hubble-ui
                port:
                  name: http
EOF
    done
    cat <<EOF >>/tmp/cluster_init_cilium_hubble_ui_ingress.yaml
  tls:
  - hosts: $HUBBLE_DASHBOARD_DNS_NAMES_JS
    secretName: hubble-cert
---
EOF
    kubectl apply -f /tmp/cluster_init_cilium_hubble_ui_ingress.yaml
    #rm /tmp/cluster_init_cilium_hubble_ui_ingress.yaml
  fi
}

function get_subnet_from_cidr() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_subnet_from_cidr cidr"
  else
    IP=$1
    echo "$(echo "$IP" | sed 's/\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\.\)\([0-9]\{1,3\}\).*$/\1\2\3/')"
  fi
}

function get_kube_dns_ip() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage get_kube_dns_ip"
  else
    KUBE_DNS=$(get_subnet_from_cidr $KUBERNETES_SERVICE_CIDR)10
    echo $KUBE_DNS
  fi
}

function set_systemd_resolve_to_k8s() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage set_systemd_resolve_to_k8s"
  else
    sudo mkdir -p /etc/systemd/resolved.conf.d
    sudo bash -c "cat >/etc/systemd/resolved.conf.d/k8s-resolve.conf" <<EOF
[Resolve]
Cache=yes
DNS=$(
      get_kube_dns_ip
    )
Domains=default.svc.cluster.local svc.cluster.local cluster.local
EOF
  fi
}

function cluster_init_create_master_haproxy_cfg() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_master_haproxy_cfg"
  else
    sudo mkdir -p /etc/haproxy/k8s/
    sudo bash -c "cat >/etc/haproxy/haproxy.cfg" <<EOF
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
defaults
    mode tcp

resolvers dns
  nameserver k8s  127.0.0.53:53
  hold valid 1s

EOF
    sudo bash -c "cat >/etc/haproxy/k8s/traefik.cfg" <<EOF
frontend traefik
        timeout client 5s
        bind :443
        bind :::443
        default_backend k8s-traefik 

backend k8s-traefik
        timeout connect 1s
        timeout server 10s
        server site traefik.kube-traefik.$EXTERNAL_CLUSTER_ZONE:443 send-proxy-v2 resolvers dns check inter 1000
EOF
    sudo bash -c "sed -i /lib/systemd/system/haproxy.service -e 's/ExecStartPre=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -c -q \$EXTRAOPTS/ExecStartPre=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -f \/etc\/haproxy\/k8s -c -q \$EXTRAOPTS/' -e 's/ExecStart=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -p \$PIDFILE \$EXTRAOPTS/ExecStart=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -f \/etc\/haproxy\/k8s -p \$PIDFILE \$EXTRAOPTS/'"
    sudo systemctl daemon-reload
    #sudo systemctl restart haproxy
  fi
}

function cluster_deploy_haproxy_config_on_members() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_deploy_haproxy_config_on_members"
  else
    cluster_run_on_all_members_as_root "mkdir -p /etc/haproxy/k8s/"
    cluster_copy_file_as_root /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg
    cluster_copy_dir_as_root /etc/haproxy/k8s /etc/haproxy/
    cluster_run_on_all_members_as_root "sed -i /lib/systemd/system/haproxy.service -e 's/ExecStartPre=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -c -q \$EXTRAOPTS/ExecStartPre=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -f \/etc\/haproxy\/k8s -c -q \$EXTRAOPTS/' -e 's/ExecStart=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -p \$PIDFILE \$EXTRAOPTS/ExecStart=\/usr\/sbin\/haproxy -Ws -f \$CONFIG -f \/etc\/haproxy\/k8s -p \$PIDFILE \$EXTRAOPTS/'"
    cluster_run_on_all_members_as_root "systemctl daemon-reload"
    #cluster_run_on_all_members_as_root "systemctl restart haproxy"
  fi
}

function cluster_init_install_longhorn_ingress() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_longhorn_ingress"
  else
    echo "Using issuer: $LONGHORN_CA_ISSUER"
    cat <<EOF >/tmp/cluster_init_install_longhorn_ingress.yaml
apiVersion: v1
kind: Secret
metadata:
  namespace: longhorn-system
  name: longhorn-ui-haproxy-credentials
data:
  $MASTER_ADMIN: $MASTER_ADMIN_PASSWORD_CRYPTB64
type: Opaque
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: longhorn-ui
  namespace: longhorn-system
  annotations:
    cert-manager.io/cluster-issuer: $LONGHORN_CA_ISSUER
    haproxy.org/auth-type: basic-auth
    haproxy.org/auth-secret: longhorn-system/longhorn-ui-haproxy-credentials
    # traefik.ingress.kubernetes.io/router.entrypoints: websecure
    # traefik.ingress.kubernetes.io/router.middlewares: kube-traefik-traefik-dashboard-auth@kubernetescrd
spec:
  ingressClassName: haproxy
  rules:
EOF
    for HOST in "${LONGHORN_DASHBOARD_DNS_NAMES[@]}"; do
      cat <<EOF >>/tmp/cluster_init_install_longhorn_ingress.yaml
    - host: $HOST
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: longhorn-frontend
                port:
                  name: http
EOF
    done
    cat <<EOF >>/tmp/cluster_init_install_longhorn_ingress.yaml
  tls:
  - hosts: $LONGHORN_DASHBOARD_DNS_NAMES_JS
    secretName: longhorn-cert
---
EOF
    kubectl apply -f /tmp/cluster_init_install_longhorn_ingress.yaml
    rm /tmp/cluster_init_install_longhorn_ingress.yaml
  fi
}

function _cluster_longhorn_helm() {
  helm repo add longhorn https://charts.longhorn.io
  helm repo update
  helm $1 longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version $LONGHORN_VERSION \
    --set persistence.defaultClassReplicaCount=$STORAGE_REPLICAS,defaultSettings.defaultReplicaCount=$STORAGE_REPLICAS,defaultSettings.defaultDataPath=$STORAGE_LOCATION
}

function cluster_init_install_longhorn() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_longhorn"
  else
    _cluster_longhorn_helm install
  fi
}

function cluster_longhorn_upgrade() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_longhorn_upgrade"
  else
    _cluster_longhorn_helm upgrade
  fi
}

function cluster_init_install_openebs() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_openebs"
  else
    helm repo add openebs https://openebs.github.io/charts
    helm repo update
    helm install openebs openebs/openebs --namespace kube-openebs --create-namespace \
      --set legacy.enabled=false \
      --set jiva.enabled=true \
      --set jiva.replicas=$STORAGE_REPLICAS \
      --set jiva.defaultStoragePath=$STORAGE_LOCATION \
      --set openebs-ndm.enabled=true \
      --set localpv-provisioner.enabled=true \
      --set nfs-provisioner.enabled=true
    kubectl patch storageclass openebs-jiva-csi-default \
      -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
    cat <<EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-rwx
  annotations:
    openebs.io/cas-type: nfsrwx
    cas.openebs.io/config: |
      - name: NFSServerType
        value: "kernel"
      - name: BackendStorageClass
        value: "openebs-hostpath"
provisioner: openebs.io/nfsrwx
reclaimPolicy: Delete
EOF
  fi
}
function build_lookbusy() {
  gcc miscellaneous/lb.c -lm -o miscellaneous/lb
}

function cluster_deploy_lookbusy() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_install_longhorn"
  else
    for MEMBER in $CLUSTER_MEMBERS; do
      ARCH=$(get_host_arch $MEMBER)
      if [ "$ARCH" == "arm64" ]; then
        scp root@$ARM_SRC:/usr/local/bin/lb root@$MEMBER:/usr/local/bin/lb
      else
        scp root@$X86_64_SRC:/usr/local/bin/lb root@$MEMBER:/usr/local/bin/lb
      fi
      scp miscellaneous/lb.service root@$MEMBER:/lib/systemd/system/lb.service
      ssh root@$MEMBER "systemctl enable lb.service --now"
    done
  fi
}

function cluster_init_create_post_install_grafana_v3() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_post_install_grafana_v3 method (install/delete)"
  else
    if [ "$1" = "delete" ]; then
      helm uninstall grafana-k8s-monitoring
    else
      NAMESPACE=$MONITORING_NAMESPACE
      helm repo add grafana https://grafana.github.io/helm-charts &&
        helm repo update grafana &&
        helm upgrade --install --create-namespace grafana-k8s-monitoring grafana/k8s-monitoring \
          --namespace "$NAMESPACE" --values - <<EOF
cluster:
  name: $GRAFANA_CLUSTER_NAME
externalServices:
  prometheus:
    host: https://prometheus-prod-22-prod-eu-west-3.grafana.net
    basicAuth:
      username: "$GRAFANA_PROMETHEUS_USERNAME"
      password: "$GRAFANA_PROMETHEUS_PASSWORD"
  loki:
    host: https://logs-prod-013.grafana.net
    basicAuth:
      username: "$GRAFANA_LOGS_USERNAME"
      password: "$GRAFANA_LOGS_PASSWORD"
metrics:
  cost:
    enabled: false
  node-exporter:
    allowList:
        - node_cpu.*
        - node_exporter_build_info
        - node_filesystem.*
        - node_memory.*
        - process_cpu_seconds_total
        - process_resident_memory_bytes
opencost:
  enabled: false
  opencost:
    exporter:
      defaultClusterId: $GRAFANA_CLUSTER_NAME
    prometheus:
      external:
        url: https://prometheus-prod-22-prod-eu-west-3.grafana.net/api/prom
EOF
    fi
  fi
}

function cluster_init_create_post_install_grafana_v2() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_post_install_grafana-v2 method (install/delete)"
  else
    if [ "$1" = "delete" ]; then
      KUBECTL_VERB=delete
      HELM_VERB=uninstall
    else
      KUBECTL_VERB=apply
      HELM_VERB="install --create-namespace"
    fi
    NAMESPACE=$MONITORING_NAMESPACE
    cat <<EOF | kubectl $KUBECTL_VERB -n $NAMESPACE -f -
apiVersion: v1
kind: Namespace
metadata:
  name: $NAMESPACE
  labels:
    name: $NAMESPACE
---
kind: ConfigMap
metadata:
  name: grafana-agent
  namespace: $NAMESPACE
apiVersion: v1
data:
  agent.yaml: |    
    metrics:
      wal_directory: /var/lib/agent/wal
      global:
        scrape_interval: 60s
        external_labels:
          cluster: cloud
      configs:
      - name: integrations
        remote_write:
        - url: https://prometheus-prod-22-prod-eu-west-3.grafana.net/api/prom/push
          basic_auth:
            username: $GRAFANA_PROMETHEUS_USERNAME
            password: $GRAFANA_PROMETHEUS_PASSWORD
        scrape_configs:
        - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          job_name: integrations/kubernetes/cadvisor
          kubernetes_sd_configs:
              - role: node
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - replacement: kubernetes.default.svc.cluster.local:443
                target_label: __address__
              - regex: (.+)
                replacement: /api/v1/nodes/\${1}/proxy/metrics/cadvisor
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: __metrics_path__
          scheme: https
          tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes
        - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          job_name: integrations/kubernetes/kubelet
          kubernetes_sd_configs:
              - role: node
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - replacement: kubernetes.default.svc.cluster.local:443
                target_label: __address__
              - regex: (.+)
                replacement: /api/v1/nodes/\${1}/proxy/metrics
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: __metrics_path__
          scheme: https
          tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes
        - job_name: integrations/kubernetes/kube-state-metrics
          kubernetes_sd_configs:
              - role: pod
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - action: keep
                regex: kube-state-metrics
                source_labels:
                  - __meta_kubernetes_pod_label_app_kubernetes_io_name
        - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          job_name: integrations/node_exporter
          kubernetes_sd_configs:
              - namespaces:
                  names:
                      - grafana
                role: pod
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - action: keep
                regex: prometheus-node-exporter.*
                source_labels:
                  - __meta_kubernetes_pod_label_app_kubernetes_io_name
              - action: replace
                source_labels:
                  - __meta_kubernetes_pod_node_name
                target_label: instance
              - action: replace
                source_labels:
                  - __meta_kubernetes_namespace
                target_label: namespace
          tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
        
    integrations:
      eventhandler:
        cache_path: /var/lib/agent/eventhandler.cache
        logs_instance: integrations
    logs:
      configs:
      - name: integrations
        clients:
        - url: https://logs-prod-013.grafana.net/loki/api/v1/push
          basic_auth:
            username: $GRAFANA_LOGS_USERNAME
            password: $GRAFANA_LOGS_PASSWORD
          external_labels:
            cluster: cloud
            job: integrations/kubernetes/eventhandler
        positions:
          filename: /tmp/positions.yaml
        target_config:
          sync_period: 10s
    
EOF
    MANIFEST_URL=https://raw.githubusercontent.com/grafana/agent/$GRAFANA_AGENT_VERSION/production/kubernetes/agent-bare.yaml
    /bin/sh -c "$(export NAMESPACE=$NAMESPACE && curl -fsSL https://raw.githubusercontent.com/grafana/agent/$GRAFANA_AGENT_VERSION/production/kubernetes/install-bare.sh)" | kubectl $KUBECTL_VERB -f -
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update prometheus-community
    if [ "$HELM_VERB" = "install" ]; then
      HELM_SET="--set image.tag=$KUBE_STATE_METRICS_VERSION"
    else
      HELM_SET=""
    fi
    helm $HELM_VERB -n $NAMESPACE ksm prometheus-community/kube-state-metrics $HELM_SET
    helm $HELM_VERB -n $NAMESPACE nodeexporter prometheus-community/prometheus-node-exporter
    cat <<EOF | kubectl $KUBECTL_VERB -n $NAMESPACE -f -
kind: ConfigMap
metadata:
  name: grafana-agent-logs
  namespace: $NAMESPACE
apiVersion: v1
data:
  agent.yaml: |    
    metrics:
      wal_directory: /tmp/grafana-agent-wal
      global:
        scrape_interval: 60s
        external_labels:
          cluster: cloud
      configs:
      - name: integrations
        remote_write:
        - url: https://prometheus-prod-22-prod-eu-west-3.grafana.net/api/prom/push
          basic_auth:
            username: $GRAFANA_PROMETHEUS_USERNAME
            password: $GRAFANA_PROMETHEUS_PASSWORD
    integrations:
      prometheus_remote_write:
      - url: https://prometheus-prod-22-prod-eu-west-3.grafana.net/api/prom/push
        basic_auth:
          username: $GRAFANA_PROMETHEUS_USERNAME
          password: $GRAFANA_PROMETHEUS_PASSWORD
      
      
    logs:
      configs:
      - name: integrations
        clients:
        - url: https://logs-prod-013.grafana.net/loki/api/v1/push
          basic_auth:
            username: $GRAFANA_LOGS_USERNAME
            password: $GRAFANA_LOGS_PASSWORD
          external_labels:
            cluster: cloud
        positions:
          filename: /tmp/positions.yaml
        target_config:
          sync_period: 10s
        scrape_configs:
        - job_name: integrations/kubernetes/pod-logs
          kubernetes_sd_configs:
            - role: pod
          pipeline_stages:
            - docker: {}
          relabel_configs:
            - source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: __host__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              replacement: \$1
              separator: /
              source_labels:
                - __meta_kubernetes_namespace
                - __meta_kubernetes_pod_name
              target_label: job
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: pod
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_container_name
              target_label: container
            - replacement: /var/log/pods/*\$1/*.log
              separator: /
              source_labels:
                - __meta_kubernetes_pod_uid
                - __meta_kubernetes_pod_container_name
              target_label: __path__
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: grafana-agent-logs
  namespace: ${NAMESPACE}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: grafana-agent-logs
  namespace: $NAMESPACE
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  - events
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: grafana-agent-logs
  namespace: $NAMESPACE
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-agent-logs
subjects:
- kind: ServiceAccount
  name: grafana-agent-logs
  namespace: ${NAMESPACE}
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: grafana-agent-logs
  namespace: ${NAMESPACE}
spec:
  minReadySeconds: 10
  selector:
    matchLabels:
      name: grafana-agent-logs
  template:
    metadata:
      labels:
        name: grafana-agent-logs
    spec:
      containers:
      - args:
        - -config.expand-env=true
        - -config.file=/etc/agent/agent.yaml
        - -server.http.address=0.0.0.0:80
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        image: grafana/agent:$GRAFANA_AGENT_VERSION
        imagePullPolicy: IfNotPresent
        name: grafana-agent-logs
        ports:
        - containerPort: 80
          name: http-metrics
        securityContext:
          privileged: true
          runAsUser: 0
        volumeMounts:
        - mountPath: /etc/agent
          name: grafana-agent-logs
        - mountPath: /var/log
          name: varlog
        - mountPath: /var/lib/docker/containers
          name: varlibdockercontainers
          readOnly: true
      serviceAccountName: grafana-agent-logs
      tolerations:
      - effect: NoSchedule
        operator: Exists
      volumes:
      - configMap:
          name: grafana-agent-logs
        name: grafana-agent-logs
      - hostPath:
          path: /var/log
        name: varlog
      - hostPath:
          path: /var/lib/docker/containers
        name: varlibdockercontainers
  updateStrategy:
    type: RollingUpdate
        
    
EOF
    if [ "$HELM_VERB" = "uninstall" ]; then
      kubectl delete namespace grafana
    else
      echo "Complete"
    fi
  fi
}
function cluster_init_create_post_install_grafana() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_create_post_install_grafana"
  else
    NAMESPACE=grafana
    cat <<EOF | kubectl apply -n $NAMESPACE -f -
apiVersion: v1
kind: Namespace
metadata:
  name: $NAMESPACE
  labels:
    name: $NAMESPACE
---
kind: ConfigMap
metadata:
  name: grafana-agent
apiVersion: v1
data:
  agent.yaml: |    
    metrics:
      wal_directory: /var/lib/agent/wal
      global:
        scrape_interval: 60s
        external_labels:
          cluster: cloud
      configs:
      - name: integrations
        remote_write:
        - url: https://prometheus-prod-22-prod-eu-west-3.grafana.net/api/prom/push
          basic_auth:
            username: $GRAFANA_PROMETHEUS_USERNAME
            password: $GRAFANA_PROMETHEUS_PASSWORD
        scrape_configs:
        - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          job_name: integrations/kubernetes/cadvisor
          kubernetes_sd_configs:
              - role: node
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - replacement: kubernetes.default.svc.cluster.local:443
                target_label: __address__
              - regex: (.+)
                replacement: /api/v1/nodes/\${1}/proxy/metrics/cadvisor
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: __metrics_path__
          scheme: https
          tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes
        - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          job_name: integrations/kubernetes/kubelet
          kubernetes_sd_configs:
              - role: node
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - replacement: kubernetes.default.svc.cluster.local:443
                target_label: __address__
              - regex: (.+)
                replacement: /api/v1/nodes/\${1}/proxy/metrics
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: __metrics_path__
          scheme: https
          tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes
        - job_name: integrations/kubernetes/kube-state-metrics
          kubernetes_sd_configs:
              - role: pod
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - action: keep
                regex: kube-state-metrics
                source_labels:
                  - __meta_kubernetes_pod_label_app_kubernetes_io_name
        - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          job_name: integrations/node_exporter
          kubernetes_sd_configs:
              - namespaces:
                  names:
                      - grafana
                role: pod
          metric_relabel_configs:
              - source_labels: [__name__]
                regex: process_resident_memory_bytes|container_cpu_usage_seconds_total|kube_pod_container_resource_requests|kube_horizontalpodautoscaler_spec_max_replicas|kubelet_pod_worker_duration_seconds_bucket|kube_deployment_spec_replicas|kube_statefulset_status_replicas_updated|namespace_workload_pod:kube_pod_owner:relabel|kubelet_cgroup_manager_duration_seconds_bucket|kube_pod_status_phase|kubelet_certificate_manager_client_expiration_renew_errors|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|kube_persistentvolumeclaim_resource_requests_storage_bytes|kubelet_running_pods|storage_operation_errors_total|kube_statefulset_status_update_revision|container_network_transmit_bytes_total|container_memory_cache|kubelet_volume_stats_inodes|kube_daemonset_status_number_misscheduled|process_cpu_seconds_total|kube_daemonset_status_current_number_scheduled|kube_statefulset_status_replicas|container_network_receive_bytes_total|kube_statefulset_status_current_revision|namespace_cpu:kube_pod_container_resource_limits:sum|kubelet_certificate_manager_server_ttl_seconds|cluster:namespace:pod_memory:active:kube_pod_container_resource_requests|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|volume_manager_total_volumes|kube_replicaset_owner|kubelet_node_config_error|container_fs_writes_bytes_total|kube_job_failed|kube_daemonset_status_updated_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_horizontalpodautoscaler_status_current_replicas|kube_node_status_condition|kube_resourcequota|kube_node_info|kube_pod_container_status_waiting_reason|kubelet_certificate_manager_client_ttl_seconds|container_network_receive_packets_dropped_total|kubelet_volume_stats_capacity_bytes|node_namespace_pod_container:container_memory_rss|kubelet_running_container_count|kubelet_cgroup_manager_duration_seconds_count|kubelet_pleg_relist_interval_seconds_bucket|kube_node_status_capacity|machine_memory_bytes|kube_pod_status_reason|kubelet_running_containers|namespace_cpu:kube_pod_container_resource_requests:sum|node_filesystem_avail_bytes|kubelet_runtime_operations_total|storage_operation_duration_seconds_count|kube_daemonset_status_number_available|kube_node_spec_taint|container_network_transmit_packets_dropped_total|container_network_transmit_packets_total|kube_job_status_active|kube_node_status_allocatable|container_memory_rss|container_fs_reads_bytes_total|container_cpu_cfs_periods_total|node_namespace_pod_container:container_memory_working_set_bytes|kubelet_pleg_relist_duration_seconds_bucket|container_network_receive_packets_total|kube_deployment_metadata_generation|kube_job_status_start_time|kube_deployment_status_replicas_updated|kube_statefulset_metadata_generation|container_fs_reads_total|kubelet_running_pod_count|namespace_memory:kube_pod_container_resource_limits:sum|kube_pod_owner|rest_client_requests_total|kubelet_pod_worker_duration_seconds_count|kubelet_server_expiration_renew_errors|kube_namespace_status_phase|kubelet_pod_start_duration_seconds_bucket|go_goroutines|kube_deployment_status_observed_generation|cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits|kubernetes_build_info|node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile|kube_pod_container_resource_limits|cluster:namespace:pod_memory:active:kube_pod_container_resource_limits|kubelet_volume_stats_inodes_used|container_memory_working_set_bytes|container_cpu_cfs_throttled_periods_total|node_filesystem_size_bytes|node_namespace_pod_container:container_memory_swap|kube_horizontalpodautoscaler_status_desired_replicas|namespace_workload_pod|container_memory_swap|kubelet_volume_stats_available_bytes|kube_statefulset_status_observed_generation|kube_deployment_status_replicas_available|kube_statefulset_status_replicas_ready|kubelet_pleg_relist_duration_seconds_count|node_namespace_pod_container:container_memory_cache|kubelet_runtime_operations_errors_total|kube_horizontalpodautoscaler_spec_min_replicas|kube_pod_info|container_fs_writes_total|namespace_memory:kube_pod_container_resource_requests:sum|kubelet_node_name|kubelet_pod_start_duration_seconds_count|kube_statefulset_replicas|kube_namespace_status_phase|container_cpu_usage_seconds_total|kube_pod_status_phase|kube_pod_start_time|kube_pod_container_status_restarts_total|kube_pod_container_info|kube_pod_container_status_waiting_reason|kube_daemonset.*|kube_replicaset.*|kube_statefulset.*|kube_job.*|kube_node.*|node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate|cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests|namespace_cpu:kube_pod_container_resource_requests:sum|node_cpu.*|node_memory.*|node_filesystem.*
                action: keep
          relabel_configs:
              - action: keep
                regex: prometheus-node-exporter.*
                source_labels:
                  - __meta_kubernetes_pod_label_app_kubernetes_io_name
              - action: replace
                source_labels:
                  - __meta_kubernetes_pod_node_name
                target_label: instance
              - action: replace
                source_labels:
                  - __meta_kubernetes_namespace
                target_label: namespace
          tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
        
    integrations:
      eventhandler:
        cache_path: /var/lib/agent/eventhandler.cache
        logs_instance: integrations
    logs:
      configs:
      - name: integrations
        clients:
        - url: https://logs-prod-013.grafana.net/loki/api/v1/push
          basic_auth:
            username: $GRAFANA_LOGS_USERNAME
            password: $GRAFANA_LOGS_PASSWORD
          external_labels:
            cluster: cloud
            job: integrations/kubernetes/eventhandler
        positions:
          filename: /tmp/positions.yaml
        target_config:
          sync_period: 10s
    
EOF
    MANIFEST_URL=https://raw.githubusercontent.com/grafana/agent/$GRAFANA_AGENT_VERSION/production/kubernetes/agent-bare.yaml
    /bin/sh -c "$(curl -fsSL https://raw.githubusercontent.com/grafana/agent/$GRAFANA_AGENT_VERSION/production/kubernetes/install-bare.sh)" | kubectl apply -f -
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    helm install ksm prometheus-community/kube-state-metrics --set image.tag=$KUBE_STATE_METRICS_VERSION -n $NAMESPACE
    helm install nodeexporter prometheus-community/prometheus-node-exporter -n $NAMESPACE
    cat <<EOF | kubectl apply -n $NAMESPACE -f -
kind: ConfigMap
metadata:
  name: grafana-agent-logs
apiVersion: v1
data:
  agent.yaml: |    
    metrics:
      wal_directory: /tmp/grafana-agent-wal
      global:
        scrape_interval: 60s
        external_labels:
          cluster: cloud
      configs:
      - name: integrations
        remote_write:
        - url: https://prometheus-prod-22-prod-eu-west-3.grafana.net/api/prom/push
          basic_auth:
            username: $GRAFANA_PROMETHEUS_USERNAME
            password: $GRAFANA_PROMETHEUS_PASSWORD
    integrations:
      prometheus_remote_write:
      - url: https://prometheus-prod-22-prod-eu-west-3.grafana.net/api/prom/push
        basic_auth:
          username: $GRAFANA_PROMETHEUS_USERNAME
          password: $GRAFANA_PROMETHEUS_PASSWORD
      
      
    logs:
      configs:
      - name: integrations
        clients:
        - url: https://logs-prod-013.grafana.net/loki/api/v1/push
          basic_auth:
            username: $GRAFANA_LOGS_USERNAME
            password: $GRAFANA_LOGS_PASSWORD
          external_labels:
            cluster: cloud
        positions:
          filename: /tmp/positions.yaml
        target_config:
          sync_period: 10s
        scrape_configs:
        - job_name: integrations/kubernetes/pod-logs
          kubernetes_sd_configs:
            - role: pod
          pipeline_stages:
            - docker: {}
          relabel_configs:
            - source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: __host__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              replacement: \$1
              separator: /
              source_labels:
                - __meta_kubernetes_namespace
                - __meta_kubernetes_pod_name
              target_label: job
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: pod
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_container_name
              target_label: container
            - replacement: /var/log/pods/*\$1/*.log
              separator: /
              source_labels:
                - __meta_kubernetes_pod_uid
                - __meta_kubernetes_pod_container_name
              target_label: __path__
        
    
EOF
    MANIFEST_URL=https://raw.githubusercontent.com/grafana/agent/$GRAFANA_AGENT_VERSION/production/kubernetes/agent-loki.yaml
    /bin/sh -c "$(curl -fsSL https://raw.githubusercontent.com/grafana/agent/$GRAFANA_AGENT_VERSION/production/kubernetes/install-bare.sh)" | sed "s/namespace: default/namespace: $NAMESPACE/" | kubectl apply -n $NAMESPACE -f -
    #workaround
    kubectl delete -n default StatefulSets/grafana-agent
  fi
}

function oci_list_compartment() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_list_compartment tenancy"
  else
    TENANCY=$1
    $OCI --profile $TENANCY iam compartment list | jq '[ .data[] | select (.name != "ManagedCompartmentForPaaS") | { name: ."name", ocid:."id"} ]'
  fi
}

function oci_list_instance_running() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_list_instance_running tenancy compartment_ocid"
  else
    TENANCY=$1
    COMPARTMENT_OCID=$2
    $OCI --profile $TENANCY compute instance list --compartment-id $COMPARTMENT_OCID | jq '[ .data[] | select(."lifecycle-state" == "RUNNING" ) | { name:."display-name",ocid:."id" } ]'

  fi
}

function oci_list_vcn() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_list_vcn tenancy compartment_ocid"
  else
    TENANCY=$1
    COMPARTMENT_OCID=$2
    $OCI --profile $TENANCY network vcn list --compartment-id $COMPARTMENT_OCID | jq '[ .data[] | {ocid:.id, name:."display-name" , cidr:."cidr-block"} ]'

  fi
}

function oci_get_all_tenancy_compartment_triplet() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_get_all_tenancy_compartment_triplet"
  else
    jq '[ .[] | { tenancy:.tenancy, "compartment-ocid":.compartments[0].ocid, "compartment-name":.compartments[0].name } ]' <./oci.json
  fi
}

function _json_to_shell() {
  cat | jq -r '(.[0] | keys_unsorted) as $keys | $keys, map([.[ $keys[] ]|tostring])[] | @sh' | tail -n +2
}

function oci_get_all_running_instance() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_get_all_running_instance"
  else
    _E=()
    for ID in $(oci_get_all_tenancy_compartment_triplet | _json_to_shell); do
      _E+=($ID)
    done
    echo -e "["
    for ((_i = 0; _i < ${#_E[@]}; _i += 3)); do
      #echo "n° $_i : ${_E[$_i]} / ${_E[$_i+1]} / ${_E[$_i+2]}"
      TENANCY=$(echo ${_E[$_i]} | tr -d '\047')
      COMPARTMENT=$(echo ${_E[$_i + 1]} | tr -d '\047')
      if [ $_i != 0 ]; then
        echo -e ", "
      fi
      $OCI --profile $TENANCY compute instance list --compartment-id $COMPARTMENT | jq --arg tenancy "$TENANCY" '.data[] | select(."lifecycle-state" == "RUNNING") | {tenancy:$tenancy,ocid:.id,name:."display-name"}'
    done
    echo "]"
  fi
}

function oci_generate_instance_list() {
  oci_get_all_running_instance >~/oci-tools/src/config/instances.json
}

function oci_action_instance() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_action_instance instance_name action (ex RESET)"
  else
    NAME=$1
    ACTION=$2
    cat ~/oci-tools/src/config/instances.json | jq --arg name $NAME '.[] | select(.name == $name) ' >/tmp/instance.json
    INSTANCE_ID=$(cat /tmp/instance.json | jq '.ocid' | sed 's/\"//g')
    TENANCY=$(cat /tmp/instance.json | jq '.tenancy' | sed 's/\"//g')
    rm /tmp/instance.json
    $OCI --profile $TENANCY compute instance action --action $ACTION --instance-id $INSTANCE_ID
  fi
}

function oci_action_all_instances() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage oci_action_all_instances action (ex RESET)"
  else
    ACTION=$1
    for MEMBER in $K8S_CLUSTER; do
      if [[ "$MEMBER" == *oci* ]]; then
        echo $MEMBER
        oci_action_instance $MEMBER $ACTION
      fi
    done
  fi
}

function dev_install_local_registry() {
  if [ "$#" -ne 1 ]; then
    ACTION=apply
  else
    ACTION=$1
  fi
  HTTPASSWD=$(printf "$DOCKER_REGISTRY_USER:$(openssl passwd -apr1 "$DOCKER_REGISTRY_PASSWORD")\n")
  HTTPASSWD_B64=$(echo -n "$HTTPASSWD" | base64)
  HAPROXYPASSWD_B64=$(openssl passwd -1 $DOCKER_REGISTRY_PASSWORD | base64)
  helm repo add twuni https://helm.twun.io
  helm repo add joxit https://helm.joxit.dev
  helm repo update
  if [[ $ACTION == "apply" ]]; then
    helm upgrade --install docker-registry twuni/docker-registry \
      --create-namespace --namespace dev-container-registry \
      --set image.tag=2.7.1 \
      --set replicaCount=1 \
      --set persistence.enabled=$DOCKER_REGISTRY_PERSISTENT,persistence.deleteEnabled=true \
      --set service.type=ClusterIP
    cluster_create_local_registry_secret dev-container-registry
    helm upgrade --install --create-namespace --namespace=dev-container-registry joxit joxit/docker-registry-ui \
      --set ui.deleteImages=true,ui.proxy=true,ui.dockerRegistryUrl=http://docker-registry.dev-container-registry.cluster.external:5000 \
      --set ui.catalogElementsLimit=1000
  else
    helm uninstall --namespace dev-container-registry docker-registry
    helm uninstall --namespace dev-container-registry joxit
  fi
  cat <<EOF | kubectl $ACTION -f -
apiVersion: v1
kind: Secret
metadata:
  namespace: dev-container-registry
  name: dev-container-registry-haproxy-credentials
data:
  $DOCKER_REGISTRY_USER: $HAPROXYPASSWD_B64
type: Opaque
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: docker-registry
  namespace: dev-container-registry
  annotations:
    cert-manager.io/cluster-issuer: $CA_COMPANY-ca-issuer
    haproxy.org/auth-type: basic-auth
    haproxy.org/auth-secret: dev-container-registry/dev-container-registry-haproxy-credentials
spec:
  ingressClassName: haproxy
  rules:
    - host: $DOCKER_REGISTRY_HOST
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: docker-registry
                port:
                  number: 5000
  tls:
  - hosts: 
      - $DOCKER_REGISTRY_HOST
    secretName: docker-registry-cert
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-registry-hosting
  namespace: kube-public
data:
  localRegistryHosting.v1: |
    host: "$DOCKER_REGISTRY_HOST"
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: docker-registry-public-ui
  namespace: dev-container-registry
  annotations:
    cert-manager.io/cluster-issuer: $CA_COMPANY-ca-issuer
    haproxy.org/auth-type: basic-auth
    haproxy.org/auth-secret: dev-container-registry/dev-container-registry-haproxy-credentials
spec:
  ingressClassName: haproxy
  rules:
    - host: $DOCKER_REGISTRY_UI_HOST
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: joxit-docker-registry-ui-user-interface
                port:
                  number: 80
  tls:
  - hosts: 
      - $DOCKER_REGISTRY_UI_HOST
    secretName: docker-registry-public-ui-cert
EOF
  # TRAEFIK_LB_IP=$(cluster_get_traefik_lb_ip)
  # sudo bash -c "echo '$TRAEFIK_LB_IP docker-registry.local' >> /etc/hosts"
  # cluster_deploy_hosts
  if [[ $ACTION == "apply" ]]; then
    cluster_init_docker_registry_ui_ingress
  else
    kubectl delete namespace dev-container-registry
  fi
}

function cluster_init_docker_registry_ui_ingress() {
  if [ "$#" -ne 0 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_init_cilium_hubble_ui_ingress"
  else
    cat <<EOF >/tmp/cluster_init_docker_registry_ui_ingress.yaml
apiVersion: v1
kind: Secret
metadata:
  namespace: dev-container-registry
  name: dev-container-registry-haproxy-credentials
data:
  $DOCKER_REGISTRY_USER: $HAPROXYPASSWD_B64
type: Opaque
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: docker-registry-ui
  namespace: dev-container-registry
  annotations:
    cert-manager.io/cluster-issuer: $CA_COMPANY-ca-issuer
    # traefik.ingress.kubernetes.io/router.middlewares: dev-container-registry-cors-header@kubernetescrd
    # traefik.ingress.kubernetes.io/router.middlewares: kube-traefik-traefik-dashboard-auth@kubernetescrd
    # traefik.ingress.kubernetes.io/router.entrypoints: websecure
spec:
  ingressClassName: haproxy
  rules:
EOF
    for HOST in "${DOCKER_REGISTRY_UI_DNS_NAMES[@]}"; do
      cat <<EOF >>/tmp/cluster_init_docker_registry_ui_ingress.yaml
    - host: $HOST
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: joxit-docker-registry-ui-user-interface
                port:
                  number: 80
EOF
    done
    cat <<EOF >>/tmp/cluster_init_docker_registry_ui_ingress.yaml
  tls:
  - hosts: $DOCKER_REGISTRY_UI_DNS_NAMES_JS
    secretName: docker-registry-ui-cert
---
EOF
    kubectl apply -f /tmp/cluster_init_docker_registry_ui_ingress.yaml
    rm /tmp/cluster_init_docker_registry_ui_ingress.yaml
  fi
}

function dev_uninstall_local_registry() {
  dev_install_local_registry delete
}

function cluster_deploy_ca_cert() {
  echo -e "$ROOT_CA" | base64 -d >/tmp/root_ca.crt
  SHA=$(openssl x509 -inform pem -in /tmp/root_ca.crt -outform DER | sha1sum | cut -f1 -d' ')
  SHA_CRT_K8S=$(openssl x509 -inform pem -in $CRT_K8S -outform DER | sha1sum | cut -f1 -d' ')
  for MEMBER in $CLUSTER_MEMBERS; do
    scp /tmp/root_ca.crt root@$MEMBER:/usr/local/share/ca-certificates/$SHA.crt
    scp $CRT_K8S root@$MEMBER:/usr/local/share/ca-certificates/$SHA_CRT_K8S.crt
    ssh root@$MEMBER "update-ca-certificates --fresh"
  done
  sudo cp /tmp/root_ca.crt /usr/local/share/ca-certificates/$SHA.crt
  sudo cp $CRT_K8S /usr/local/share/ca-certificates/$SHA_CRT_K8S.crt
  sudo update-ca-certificates
  rm /tmp/root_ca.crt
}

function cluster_init_get_cilium_cli() {
  CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/master/stable.txt)
  CLI_ARCH=amd64
  if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
  curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
  sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
  sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
  rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
}

function cluster_reset_remove_oci_dns_issuer() {
  helm uninstall --namespace kube-certmanager cert-manager-webhook-oci
  kubectl delete -n kube-certmanager secret/oci-profile
  kubectl delete -n kube-certmanager secret/letsencrypt-oci
  kubectl delete -n kube-certmanager secret/letsencrypt-staging-oci
  kubectl delete -n kube-certmanager secret/cert-manager-webhook-oci-ca
  kubectl delete ClusterIssuer/letsencrypt-staging-oci
  kubectl delete ClusterIssuer/letsencrypt-oci
}

function cluster_init_install_oci_dns_issuer() {
  helm repo add highcanfly https://helm-repo.highcanfly.club/
  helm repo update
  helm install --namespace kube-certmanager cert-manager-webhook-oci highcanfly/cert-manager-webhook-oci
  cat <<EOF | kubectl apply -n kube-certmanager -f -
apiVersion: v1
kind: Secret
metadata:
  namespace: kube-certmanager
  name: oci-profile
type: Opaque
data:
  privateKey: $OCI_PRIVATE_KEY
stringData:
  tenancy: "$OCI_TENANCY"
  user: "$OCI_USER"
  region: "$OCI_REGION"
  fingerprint: "$OCI_FINGERPRINT" 
  privateKeyPassphrase: ""
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging-oci
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: $LETSENCRYPT_USER
    privateKeySecretRef:
      name: letsencrypt-staging-oci
    solvers:
      - dns01:
          webhook:
            groupName: acme.d-n.be
            solverName: oci
            config:
              ociProfileSecretName: oci-profile
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-oci
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: $LETSENCRYPT_USER
    privateKeySecretRef:
      name: letsencrypt-oci
    solvers:
      - dns01:
          webhook:
            groupName: acme.d-n.be
            solverName: oci
            config:
              ociProfileSecretName: oci-profile
EOF
}

function azure_install_cli() {
  sudo mkdir -p /etc/apt/keyrings
  curl -sLS https://packages.microsoft.com/keys/microsoft.asc |
    gpg --dearmor |
    sudo tee /etc/apt/keyrings/microsoft.gpg >/dev/null
  sudo chmod go+r /etc/apt/keyrings/microsoft.gpg
  AZ_REPO=$(lsb_release -cs)
  echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main" |
    sudo tee /etc/apt/sources.list.d/azure-cli.list
  sudo apt-get update
  sudo apt-get install azure-cli
}

function cluster_reset_remove_azure_dns_issuer() {
  kubectl delete -n kube-certmanager secret/letsencrypt-azure
  kubectl delete -n kube-certmanager secret/letsencrypt-staging-azure
  kubectl delete ClusterIssuer/letsencrypt-staging-azure
  kubectl delete ClusterIssuer/letsencrypt-azure
}

function cluster_init_azure_dns_issuer() {
  if [ "$#" -ne 1 ]; then
    ACTION=apply
  else
    ACTION=$1
  fi
  cat <<EOF | kubectl $ACTION -f -
apiVersion: v1
kind: Secret
metadata:
  name: azuredns-config
  namespace: kube-certmanager
type: Opaque
stringData:
  azure-client-secret: "$AZURE_CERT_MANAGER_SP_PASSWORD"
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-azure
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: $LETSENCRYPT_USER
    privateKeySecretRef:
      name: letsencrypt-azure
    solvers:
    - dns01:
        azureDNS:
          clientID: $AZURE_CERT_MANAGER_SP_APP_ID
          clientSecretSecretRef:
            name: azuredns-config
            key: azure-client-secret
          subscriptionID: $AZURE_SUBSCRIPTION_ID
          tenantID: $AZURE_TENANT_ID
          resourceGroupName: $AZURE_DNS_ZONE_RESOURCE_GROUP
          hostedZoneName: $AZURE_DNS_ZONE
          environment: AzurePublicCloud
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-staging-azure
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: $LETSENCRYPT_USER
    privateKeySecretRef:
      name: letsencrypt-staging-azure
    solvers:
    - dns01:
        azureDNS:
          clientID: $AZURE_CERT_MANAGER_SP_APP_ID
          clientSecretSecretRef:
            name: azuredns-config
            key: azure-client-secret
          subscriptionID: $AZURE_SUBSCRIPTION_ID
          tenantID: $AZURE_TENANT_ID
          resourceGroupName: $AZURE_DNS_ZONE_RESOURCE_GROUP
          hostedZoneName: $AZURE_DNS_ZONE
          environment: AzurePublicCloud
EOF
}
function cluster_init_cloudflare_dns_issuer() {
  if [ "$#" -ne 1 ]; then
    ACTION=apply
  else
    ACTION=$1
  fi
  cat <<EOF | kubectl $ACTION -f -
apiVersion: v1
kind: Secret
metadata:
  name: cloudflare-api-key-secret
  namespace: kube-certmanager
type: Opaque
stringData:
  api-key: $CF_API_KEY
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  namespace: kube-certmanager
  name: letsencrypt-cloudflare
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: $LETSENCRYPT_USER
    privateKeySecretRef:
      name: letsencrypt-cloudflare
    solvers:
    - dns01:
        cloudflare:
          email: $CF_API_EMAIL
          apiKeySecretRef:
            name: cloudflare-api-key-secret
            key: api-key
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  namespace: kube-certmanager
  name: letsencrypt-cloudflare-staging
spec:
  acme:
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    email: $LETSENCRYPT_USER
    privateKeySecretRef:
      name: letsencrypt-cloudflare-staging
    solvers:
    - dns01:
        cloudflare:
          email: $CF_API_EMAIL
          apiKeySecretRef:
            name: cloudflare-api-key-secret
            key: api-key
EOF
}

function cluster_cloudflare_external_dns() {
  if [ "$#" -ne 1 ]; then
    ACTION=apply
  else
    ACTION=$1
  fi
  cat <<EOF >/tmp/cloudflare_external_dns.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: kube-system
  name: external-dns
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: external-dns
rules:
- apiGroups: [""]
  resources: ["services","endpoints","pods"]
  verbs: ["get","watch","list"]
- apiGroups: ["extensions","networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get","watch","list"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: external-dns-viewer
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: external-dns
subjects:
- kind: ServiceAccount
  name: external-dns
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: kube-system
  name: external-dns
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: external-dns
  template:
    metadata:
      labels:
        app: external-dns
    spec:
      serviceAccountName: external-dns
      containers:
      - name: external-dns
        image: registry.k8s.io/external-dns/external-dns:v0.13.4
        args:
EOF
  for DOMAIN in "${CF_DOMAINS[@]}"; do
    cat <<EOF >>/tmp/cloudflare_external_dns.yaml
        - --domain-filter=$DOMAIN
EOF
  done
  cat <<EOF >>/tmp/cloudflare_external_dns.yaml
        - --source=ingress # service is also possible
        - --provider=cloudflare
        - --log-level=debug
        env:
        - name: CF_API_KEY
          value: "$CF_API_KEY"
        - name: CF_API_EMAIL
          value: "$CF_API_EMAIL"
EOF
  kubectl $ACTION -f /tmp/cloudflare_external_dns.yaml
  rm /tmp/cloudflare_external_dns.yaml
}

function oci-manage-copyright() {
  cat <<EOF
oci-manage -- A library providing functions for building and managing
a bare metal Kubernetes cluster on Oracle OCI

Feel free to contribute to this project at:
   https://github.com/eltorio/oci-manage

Copyright 2023 Ronan Le Meillat.

oci-manage is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

oci-manage is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with oci-manage.  If not, see <https://www.gnu.org/licenses/agpl-3.0.html>.
EOF
}

function cluster_convert_pem_to_base64() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_convert_pem_to_base64 certificate_or_key"
  else
    PEM=$1
    cat $PEM | base64 | tr -d '\n'
  fi
}

function cluster_haproxy_stop() {
  cluster_run_on_all_as_root "systemctl stop haproxy ; systemctl stop haproxy-ingress ; killall haproxy ; killall haproxy-ingress-controller"
}

function cluster_haproxy_start() {
  cluster_run_on_all_as_root "systemctl start haproxy ; systemctl start haproxy-ingress"
}

function cluster_haproxy_restart() {
  cluster_haproxy_stop
  sleep 5
  cluster_haproxy_start
}

function cluster_haproxy_reload() {
  cluster_run_on_all_as_root "systemctl reload haproxy ; systemctl reload haproxy-ingress"
}

function cluster_haproxy_k8s_status() {
  cluster_run_on_all_as_root "curl -k -s -I https://k8s-api.private:6443/livez"
}

function cluster_haproxy_status() {
  cluster_run_on_all_as_root "systemctl status haproxy-ingress"
}
function cluster_coredns_get_corefile() {
  kubectl get configmap -n kube-system coredns -o json | jq .data.Corefile | sed -e 's/\\n/\n/g' -e 's/"//g'
}

function cluster_coredns_add_k8s_external() {
  cluster_get_coredns_corefile | grep -qe '^\s*k8s_external '
  NOTALREADYDONE=$?
  if [ $NOTALREADYDONE -eq 1 ]; then
    cluster_get_coredns_corefile | sed "/k8s_external $EXTERNAL_CLUSTER_ZONE/a loadbalance"
  else
    cluster_get_coredns_corefile
  fi
}

function cluster_init_coredns_config() {
  cat <<EOF | kubectl apply -f -
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
        k8s_external $EXTERNAL_CLUSTER_ZONE
    }
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
EOF
}

function cluster_iptables_open_ipv4_port() {
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_iptables_open_ipv4_port port tcp/udp comment"
  else
    PORT=$1
    PROTOCOL=$2
    COMMENT=$3
    cat /etc/iptables/rules.v4 | grep -qe "^-A INPUT.*-p $PROTOCOL.*--dport $PORT.*-j ACCEPT.*$"
    NOTALREADYDONE=$?
    if [ $NOTALREADYDONE -eq 1 ]; then
      cat /etc/iptables/rules.v4 | sed "/^-A INPUT.*-p tcp.*--dport 22.*-j ACCEPT.*$/a -A INPUT -p $PROTOCOL -m state --state NEW -m $PROTOCOL --dport $PORT -j ACCEPT -m comment --comment \"$COMMENT\"" >/tmp/rules.v4
      sudo cp /etc/iptables/rules.v4 /etc/iptables/rules.v4.bak
      sudo cp /tmp/rules.v4 /etc/iptables/rules.v4
      cp /etc/iptables/rules.v4 iptables/
      init_set_iptables_members
    else
      echo "already open"
    fi
  fi
}

function cluster_install_helm_dashboard_ingress() {
  if [ "$#" -ne 1 ]; then
    ACTION=apply
  else
    ACTION=$1
  fi
  cat <<EOF >/tmp/helm-dashboard.yaml
apiVersion: v1
kind: Secret
metadata:
  namespace: helm-dashboard
  name: helm-dashboard-haproxy-credentials
data:
  $MASTER_ADMIN: $MASTER_ADMIN_PASSWORD_CRYPTB64
type: Opaque
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: helm-dashboard
  namespace: helm-dashboard
  annotations:
    cert-manager.io/cluster-issuer: $CA_COMPANY-ca-issuer
    haproxy.org/auth-type: basic-auth
    haproxy.org/auth-secret: helm-dashboard/helm-dashboard-haproxy-credentials
    # traefik.ingress.kubernetes.io/router.entrypoints: websecure
    # traefik.ingress.kubernetes.io/router.middlewares: kube-traefik-traefik-dashboard-auth@kubernetescrd
spec:
  ingressClassName: haproxy
  rules:
EOF
  for HOST in "${HELM_DASHBOARD_DNS_NAMES[@]}"; do
    cat <<EOF >>/tmp/helm-dashboard.yaml
    - host: $HOST
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: helm-dashboard
                port:
                  number: 8080
EOF
  done
  cat <<EOF >>/tmp/helm-dashboard.yaml
  tls:
  - hosts: $HELM_DASHBOARD_DNS_NAMES_JS
    secretName: helm-cert
---
EOF
  kubectl $ACTION -f /tmp/helm-dashboard.yaml
  rm /tmp/helm-dashboard.yaml

}

function cluster_install_helm_dashboard() {
  helm repo add highcanfly https://helm-repo.highcanfly.club/
  helm repo update
  helm upgrade --install --namespace helm-dashboard --create-namespace helm-dashboard highcanfly/helm-dashboard
}

function cluster_create_ingress_cloudflare() {
  if [ "$#" -ne 4 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_create_ingress_cloudflare host namespace service port"
  else
    HOST=$1
    NAMESPACE=$2
    SERVICE=$3
    PORT=$4
    cat <<EOF | kubectl -n $NAMESPACE apply -f -
apiVersion: v1
kind: Secret
metadata:
  namespace: $NAMESPACE
  name: $NAMESPACE-haproxy-credentials
data:
  $MASTER_ADMIN: $MASTER_ADMIN_PASSWORD_CRYPTB64
type: Opaque
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: $SERVICE-ingress
  namespace: $NAMESPACE
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-cloudflare
    # traefik.ingress.kubernetes.io/router.entrypoints: websecure
    haproxy.org/auth-type: basic-auth
    haproxy.org/auth-secret: $NAMESPACE/$NAMESPACE-haproxy-credentials
    external-dns.alpha.kubernetes.io/target: $CLUSTER_HA_FQDN
    external-dns.alpha.kubernetes.io/hostname: $HOST
    external-dns.alpha.kubernetes.io/ttl: "86400"
spec:
  ingressClassName: haproxy
  rules:
  - host: $HOST
    http:
      paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: $SERVICE
              port:
                number: $PORT
  tls:
  - hosts: [$HOST]
    secretName: $HOST-tls-cert
EOF
  fi
}

function cluster_create_local_registry_secret() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_create_local_registry_secret namespace"
  else
    NAMESPACE=$1
    kubectl create secret -n $NAMESPACE docker-registry local-registry-credential --docker-server=$DOCKER_REGISTRY_HOST --docker-username=$DOCKER_REGISTRY_USER --docker-password=$DOCKER_REGISTRY_PASSWORD --docker-email=none@example.org
  fi
}

function cluster_create_egress_config() {
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_create_egress_config gateway namespace pod"
  else
    GATEWAY=$1
    NAMESPACE=$2
    POD=$3
    cat <<EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumEgressGatewayPolicy
metadata:
  name: egress-$GATEWAY
  namespace: $NAMESPACE
spec:
  selectors:
  - podSelector:
      matchLabels:
        io.kompose.service: $POD
  destinationCIDRs:
  - "0.0.0.0/0"

  egressGateway:
    nodeSelector:
      matchLabels:
        kubernetes.io/hostname: $GATEWAY
    interface: eth0
EOF
  fi
}

function cluster_install_haproxy_2_7_single() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_install_haproxy_2_7_single host"
  else
    ssh root@$1 "apt remove -y --purge haproxy"
    ssh root@$1 "apt-get install -y --no-install-recommends software-properties-common && add-apt-repository ppa:vbernat/haproxy-2.7 && apt-get install -y haproxy=2.7.\* && apt autoremove -y && setcap cap_net_bind_service=+ep /usr/sbin/haproxy"

  fi
}
function cluster_install_haproxy_2_7() {
  for MEMBER in $K8S_CLUSTER; do
    echo $MEMBER
    ssh root@$MEMBER "apt remove -y --purge haproxy"
    ssh root@$MEMBER "apt-get install -y --no-install-recommends software-properties-common && add-apt-repository ppa:vbernat/haproxy-2.7 && apt-get install -y haproxy=2.7.\* && apt autoremove -y && setcap cap_net_bind_service=+ep /usr/sbin/haproxy"
  done
}

function get_install_haproxy_ingress_controller() {
  # URL="https://github.com/haproxytech/kubernetes-ingress/releases/download/v$HAPROXY_INGRESS_CONTROLLER_VERSION/haproxy-ingress-controller_"$HAPROXY_INGRESS_CONTROLLER_VERSION"_Linux_$(dpkg --print-architecture).tar.gz"
  sudo systemctl stop haproxy-ingress
  # curl -fsSL $URL | sudo tar -C /usr/local/bin/ --overwrite -xvz haproxy-ingress-controller
  cd "$HOME" || exit
  git clone https://github.com/eltorio/kubernetes-ingress &&
    cd kubernetes-ingress &&
    go mod tidy -v &&
    go build -v . &&
    ls -lh kubernetes-ingress &&
    sudo cp kubernetes-ingress /usr/local/bin/haproxy-ingress-controller
  cd "$HOME" && rm -rf kubernetes-ingress
  kubectl delete -n kube-system configmap haproxy-kubernetes-ingress
  kubectl delete -n kube-system configmap haproxy-kubernetes-tcpservices
  kubectl create -n kube-system configmap haproxy-kubernetes-ingress
  kubectl create -n kube-system configmap haproxy-kubernetes-tcpservices
}

function cluster_create_haproxy_ingress_class() {
  cat <<EOF | kubectl apply -n kube-system -f -
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: haproxy
spec:
  controller: haproxy.org/ingress-controller
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: GatewayClass
metadata:
  name: haproxy-gatewayclass
spec:
  controllerName: "haproxy.org/gateway-controller"
EOF

}

function cluster_install_haproxy_ingress_controller() {
  get_install_haproxy_ingress_controller
  cat <<EOF >/tmp/haproxy-ingress.service
[Unit]
Description="HAProxy Kubernetes Ingress Controller"
Documentation=https://www.haproxy.com/
Requires=network-online.target
After=network-online.target

[Service]
Type=simple
User=root
Group=root
ExecStart=/usr/local/bin/haproxy-ingress-controller \
            --gateway-controller-name=haproxy.org/gateway-controller \
            --configmap-tcp-services=kube-system/haproxy-kubernetes-tcpservices \
            --configmap=kube-system/haproxy-kubernetes-ingress \
            --program=/usr/sbin/haproxy \
            --ipv4-bind-address=0.0.0.0 \
            --http-bind-port=80 \
            --https-bind-port=443 \
            --config-dir=/etc/haproxy-ingress \
            --external \
            --log=debug
ExecReload=/bin/kill --signal HUP \$MAINPID
KillMode=process
KillSignal=SIGTERM
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
  for MEMBER in $CLUSTER_MEMBERS; do
    echo $MEMBER
    cluster_run_on_all_as_root "mkdir -p /etc/haproxy-ingress"
    ssh root@$MEMBER "systemctl stop haproxy && systemctl disable haproxy && systemctl stop haproxy-ingress"
    scp /tmp/haproxy-ingress.service root@$MEMBER:/lib/systemd/system/haproxy-ingress.service
    scp /usr/local/bin/haproxy-ingress-controller root@$MEMBER:/usr/local/bin/haproxy-ingress-controller
    ssh root@$MEMBER "systemctl daemon-reload && systemctl enable haproxy-ingress && systemctl start haproxy-ingress"
  done
  echo "Control plane"
  sudo mv /tmp/haproxy-ingress.service /lib/systemd/system/haproxy-ingress.service
  sudo systemctl stop haproxy
  sudo systemctl disable haproxy
  sudo systemctl daemon-reload
  sudo systemctl enable haproxy-ingress
  sudo systemctl start haproxy-ingress
}

function member_enable_sysctl_value() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage member_enable_sysctl_value host sysctl_parameter"
  else
    HOST=$1
    SYSPARAM=$2
    ssh root@$HOST "grep -qF \"$SYSPARAM\" /etc/sysctl.conf  || echo \"$SYSPARAM = 1\" >> /etc/sysctl.conf"
    ssh root@$HOST "sed -ibak -r 's/#{1,}?$SYSPARAM.*/$SYSPARAM = 1/g' /etc/sysctl.conf && sysctl -w $SYSPARAM=1"
  fi
}

function member_disable_sysctl_value() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage member_disable_sysctl_value host sysctl_parameter"
  else
    HOST=$1
    SYSPARAM=$2
    ssh root@$HOST "grep -qF \"$SYSPARAM\" /etc/sysctl.conf  || echo \"$SYSPARAM = 0\" >> /etc/sysctl.conf"
    ssh root@$HOST "sed -ibak -r 's/#{1,}?$SYSPARAM.*/$SYSPARAM = 0/g' /etc/sysctl.conf && sysctl -w $SYSPARAM=0"
  fi
}

function cluster_enable_sysctl_value() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_enable_sysctl_value sysctl_parameter"
  else
    SYSPARAM=$1
    for MEMBER in $K8S_CLUSTER; do
      member_enable_sysctl_value $MEMBER $SYSPARAM
    done
  fi
}

function cluster_disable_sysctl_value() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_disable_sysctl_value sysctl_parameter"
  else
    SYSPARAM=$1
    cluster_run_on_all_as_root "grep -qF \"$SYSPARAM\" /etc/sysctl.conf  || echo \"$SYSPARAM = 0\" >> /etc/sysctl.conf"
    cluster_run_on_all_as_root "sed -ibak -r 's/#{1,}?$SYSPARAM.*/$SYSPARAM = 0/g' /etc/sysctl.conf && sysctl -w $SYSPARAM=0"
  fi
}

function cluster_enable_ipv4_forward() {
  #cluster_run_on_all_as_root "sed -ibak -r 's/#{1,}?net.ipv4.ip_forward.*/net.ipv4.ip_forward = 1/g' /etc/sysctl.conf && sysctl -w net.ipv4.ip_forward=1"
  cluster_enable_sysctl_value net.ipv4.ip_forward
}

function cluster_enable_ipv6_forward() {
  #cluster_run_on_all_as_root "sed -ibak -r 's/#{1,}?net.ipv6.conf.all.forwarding.*/net.ipv6.conf.all.forwarding = 1/g' /etc/sysctl.conf && sysctl -w net.ipv6.conf.all.forwarding=1"
  cluster_enable_sysctl_value net.ipv6.conf.all.forwarding
}

function cluster_enable_bridge_nf_call_iptables() {
  #cluster_run_on_all_as_root "sed -ibak -r 's/#{1,}?net.bridge.bridge-nf-call-iptables.*/net.bridge.bridge-nf-call-iptables = 1/g' /etc/sysctl.conf && sysctl -w net.bridge.bridge-nf-call-iptables=1"
  cluster_enable_sysctl_value net.bridge.bridge-nf-call-iptables
}

function cluster_enable_bridge_nf_call_ip6tables() {
  #cluster_run_on_all_as_root "sed -ibak -r 's/#{1,}?net.bridge.bridge-nf-call-ip6tables.*/net.bridge.bridge-nf-call-ip6tables = 1/g' /etc/sysctl.conf && sysctl -w net.bridge.bridge-nf-call-ip6tables=1"
  cluster_enable_sysctl_value net.bridge.bridge-nf-call-ip6tables
}

#grep -qF "net.ipv4.ip_forward" /etc/sysctl.conf  || echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
function cluster_remove_completed_pods() {
  kubectl delete jobs -A --field-selector status.successful=1
}

function dev_local_registry_get_podname() {
  echo $(kubectl -n dev-container-registry get pods | awk '$1 ~ /docker-registry-[0-9a-]+/ && $3=="Running" {print $1}')
}
function dev_local_registry_garbage_collect() {
  POD_NAME=$(dev_local_registry_get_podname)
  kubectl exec -n dev-container-registry -it $POD_NAME -- /bin/registry garbage-collect /etc/docker/registry/config.yml
}

function wg_meshconf_apt_dist_upgrade() {
  MEMBERS=$(wg_meshconf_get_all_peers_sorted)
  for MEMBER in $MEMBERS; do
    echo "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
    echo "RUNNING for $MEMBER"
    ssh root@$MEMBER "NEEDRESTART_MODE=a apt update && apt -y dist-upgrade"
  done
}

function cluster_haproxy_get_validation_script() {
  for MEMBER in $K8S_CLUSTER; do
    echo "echo \"test for $MEMBER\" && curl -k -6 https://$MEMBER.oci.sctg.eu.org && curl -k -4 https://$MEMBER.oci.sctg.eu.org && echo \"+++\""
  done
}

function cluster_delete_stucked_namespace() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_delete_stucked_namespace namespace"
  else
    NAMESPACE=$1
    kubectl proxy &
    kubectl get namespace $NAMESPACE -o json | jq '.spec = {"finalizers":[]}' >temp.json
    curl -k -H "Content-Type: application/json" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize
  fi
}

function cluster_delete_stucked_pods() {
  if [ "$1" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_delete_stucked_pods namespace pod_basename"
  else
    NAMESPACE=$1
    POD=$2
    PODS=$(kubectl -n $NAMESPACE get pods | grep "$POD" | cut -d' ' -f1)
    echo "stucked pods: $PODS"
    for _POD in $PODS; do
      echo "deleting $_POD"
      kubectl delete pod $_POD --grace-period=0 --force --namespace $NAMESPACE
    done
  fi
}

function cluster_delete_stucked_pvc() {
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_delete_stucked_pvc namespace pvc pv"
  else
    NAMESPACE=$1
    PVC=$2
    PV=$3
    kubectl patch -n $NAMESPACE pv/$PV -p '{"metadata":{"finalizers":null}}'
    kubectl patch -n $NAMESPACE pvc/$PVC -p '{"metadata":{"finalizers":null}}'
    kubectl delete -n $NAMESPACE pvc/$PVC --grace-period=0 --force
    kubectl delete -n $NAMESPACE pv/$PV --grace-period=0 --force
  fi
}

function cluster_longhorn_patch_crd_finalizers() {
  kubectl -n longhorn-system patch crd/nodes.longhorn.io -p '{"metadata":{"finalizers":null}}'
  kubectl -n longhorn-system patch crd/replicas.longhorn.io -p '{"metadata":{"finalizers":null}}'
  kubectl -n longhorn-system patch crd/sharemanagers.longhorn.io -p '{"metadata":{"finalizers":null}}'
  kubectl -n longhorn-system patch crd/engineimages.longhorn.io -p '{"metadata":{"finalizers":null}}'
  kubectl -n longhorn-system patch crd/engines.longhorn.io -p '{"metadata":{"finalizers":null}}'
  kubectl -n longhorn-system patch crd/snapshots.longhorn.io -p '{"metadata":{"finalizers":null}}'
  kubectl -n longhorn-system patch crd/volumes.longhorn.io -p '{"metadata":{"finalizers":null}}'
  kubectl -n longhorn-system patch crd/volumeattachments.longhorn.io -p '{"metadata":{"finalizers":null}}'
}

function cluster_install_weblate() {
  helm repo add weblate https://helm.weblate.org
  helm repo update weblate
  cat <<EOF >/tmp/weblate.yaml
debug: '1'
persistence:
  size: 10Gi
  accessMode: ReadWriteMany
image:
  tag: '5.3'
replicaCount: 1
adminEmail: '$WEBLATE_EMAIL'
adminUser: '$WEBLATE_ADMIN'
adminPassword: '$WEBLATE_PASSWORD'
siteDomain: 'weblate.$AZURE_DNS_ZONE'
defaultFromEmail: '$WEBLATE_EMAIL_SENDER'
emailHost: '$WEBLATE_EMAIL_SENDER_HOST'
emailPassword: '$WEBLATE_EMAIL_SENDER_PASSWORD'
emailUser: '$WEBLATE_EMAIL_SENDER'
serverEmail: $WEBLATE_EMAIL_SENDER'
configOverride: |
  REGISTRATION_EMAIL_MATCH = $WEBLATE_REGISTRATION_EMAIL_MATCH
  WEBLATE_GPG_IDENTITY = "$WEBLATE_GPG_IDENTITY"
  ENABLE_HTTPS = True
  SESSION_COOKIE_SECURE = ENABLE_HTTPS
  SECURE_SSL_REDIRECT = ENABLE_HTTPS
  SECURE_HSTS_PRELOAD = ENABLE_HTTPS
  SECURE_HSTS_INCLUDE_SUBDOMAINS = ENABLE_HTTPS
  SECURE_HSTS_SECONDS = 3600
  BORG_EXTRA_ARGS = ["--exclude", "lost+found","--exclude", "/app/data/lost+found"]
emailSSL: false
emailTLS: true
livenessProbe:
  initialDelaySeconds: 120
readinessProbe:
  initialDelaySeconds: 120
ingress:
  enabled: true
  ingressClassName: haproxy
  annotations:
    cert-manager.io/cluster-issuer:  letsencrypt-azure #$CA_COMPANY-ca-issuer
  hosts:
  - host: weblate.$AZURE_DNS_ZONE
    paths:
    - path: /
      pathType: Prefix
  tls:
     - secretName: weblate-tls
       hosts:
         - weblate.$AZURE_DNS_ZONE
# postgresql:
#   image:
#     debug: true
#     registry: docker.io
#     tag: 15-to-16
#     repository: highcanfly/postgres-upgrade
#     pullPolicy: Always
#   diagnosticMode:
#     enabled: true
#   securityContext:
#     capabilities: {}
#     runAsUser: 0
EOF
  helm upgrade --install --namespace weblate --create-namespace weblate weblate/weblate --values /tmp/weblate.yaml
  #helm template --debug --dry-run --namespace weblate --create-namespace weblate weblate/weblate --values /tmp/weblate.yaml
}

function cluster_lb_start() {
  cluster_run_on_all_as_root "systemctl start lb"

}

function cluster_lb_stop() {
  cluster_run_on_all_as_root "systemctl stop lb"
}

function cluster_init_install_kubevirt() {
  KUBEVIRT_VERSION=$(curl -L https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest-arm64)
  echo "Installing Kubevirt $KUBEVIRT_VERSION"
  kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${KUBEVIRT_VERSION}/kubevirt-operator-arm64.yaml
  kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${KUBEVIRT_VERSION}/kubevirt-cr-arm64.yaml
  kubectl patch --namespace kubevirt KubeVirt kubevirt --type=merge -p '{"spec":{"configuration":{"developerConfiguration":{"useEmulation":true}}}}'
  kubectl create -n kubevirt secret generic version --from-literal=version=$KUBEVIRT_VERSION
  KUBEVIRT_VERSION_FULL=$(kubectl get kubevirt.kubevirt.io/kubevirt -n kubevirt -o=jsonpath="{.status.observedKubeVirtVersion}")
  curl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/v1.0.0/virtctl-v1.0.0-linux-arm64
  chmod +x virtctl
  sudo install virtctl /usr/local/bin
  rm virtctl
}

function cluster_reset_remove_kubevirt() {
  KUBEVIRT_VERSION=$(kubectl get -n kubevirt secret version -o jsonpath='{.data.version}' | base64 --decode)
  echo "Uninstalling Kubevirt $KUBEVIRT_VERSION"
  kubectl delete -n kubevirt secret version
  kubectl delete -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${KUBEVIRT_VERSION}/kubevirt-cr-arm64.yaml
  kubectl delete -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${KUBEVIRT_VERSION}/kubevirt-operator-arm64.yaml
  sudo rm /usr/local/bin/virtctl
}

function cluster_install_qemu_x86_64_for_build() {
  cluster_run_on_all_as_root "docker run --privileged --rm tonistiigi/binfmt --install x86_64"
}

function cluster_setup_change_listener() {
  sudo sed -i 's/--bind-address=127.0.0.1/--bind-address=0.0.0.0/' /etc/kubernetes/manifests/kube-controller-manager.yaml
  sudo sed -i 's/--bind-address=127.0.0.1/--bind-address=0.0.0.0/' /etc/kubernetes/manifests/kube-scheduler.yaml
  sudo sed -i "s#--listen-metrics-urls=.*#--listen-metrics-urls=http://127.0.0.1:2381,http://$CONTROL_PLANE_IP:2381#" /etc/kubernetes/manifests/etcd.yaml
}

function cluster_init_create_post_install_prometheus() {
  API_TOKEN=$(cat /proc/sys/kernel/random/uuid)
  echo "ATTENTION password for apitoken='$API_TOKEN'"
  echo "you will never see it, please note it"
  cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: "$(date --iso-8601=seconds --utc)"
  labels:
    kubernetes.io/metadata.name: $MONITORING_NAMESPACE
  name: $MONITORING_NAMESPACE
---
apiVersion: v1
kind: Secret
metadata:
  namespace: $MONITORING_NAMESPACE
  name: prometheus-haproxy-credentials
data:
  $MASTER_ADMIN: $MASTER_ADMIN_PASSWORD_CRYPTB64
  apitoken: $(openssl passwd -1 "$API_TOKEN" | base64)
type: Opaque
---
EOF
  #helm template --namespace $MONITORING_NAMESPACE prometheus prometheus-community/kube-prometheus-stack  --values - <<EOF
  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  helm repo update prometheus-community
  UUID=$(cat /proc/sys/kernel/random/uuid)
  mkdir -p /tmp/$UUID
  cat <<EOF >/tmp/$UUID/values.yaml
prometheus:
  ingress:
    enabled: true
    ingressClassName: haproxy
    annotations:
      haproxy.org/auth-type: basic-auth
      haproxy.org/auth-secret: $MONITORING_NAMESPACE/prometheus-haproxy-credentials
      cert-manager.io/cluster-issuer: $CA_COMPANY-ca-issuer
    hosts:
    - prometheus.$AZURE_DNS_ZONE
    tls:
    - secretName: prometheus-crt
      hosts:
        - prometheus.$AZURE_DNS_ZONE
grafana:
  downloadDashboardsImage:
    registry:	docker.io
    repository:	highcanfly/net-tools
    tag: latest
  adminPassword: "$MASTER_ADMIN_PASSWORD"
  sidecar:
    dashboards:
      enabled: false
    datasources:
      enabled: false
  ingress:
    enabled: true
    ingressClassName: haproxy
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-azure
    hosts:
    - grafana.$AZURE_DNS_ZONE
    tls:
    - secretName: grafana-crt
      hosts:
        - grafana.$AZURE_DNS_ZONE
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'grafana-dashboards-kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes
      - name: 'grafana-dashboards-kubernetes-integration'
        orgId: 1
        folder: 'Integration'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes-integration
      - name: 'grafana-dashboards-cilium'
        orgId: 1
        folder: 'Integration'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/grafana-dashboards-cilium
  dashboards:
    grafana-dashboards-kubernetes:
      k8s-system-api-server:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json
        token: ''
      k8s-system-coredns:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json
        token: ''
      k8s-views-global:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
        token: ''
      k8s-views-namespaces:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
        token: ''
      k8s-views-nodes:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
        token: ''
      k8s-views-pods:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json
        token: ''
      k8s-view-pvc:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/k8s-pvc.json
        token: ''
      prometheus:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-addons-prometheus.json
        token: ''
    grafana-dashboards-kubernetes-integration:
      home:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/home.json
        token: ''
      multi:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/multi-cluster.json
        token: ''
      cluster:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/cluster.json
        token: ''
      namespace-pods:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/namespace-pods.json
        token: ''
      namespace-workloads:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/namespace-workloads.json
        token: ''
      node-pods:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/node-pods.json
        token: ''
      pod:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/pod.json
        token: ''
      workload:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/workload.json
        token: ''
      kubelet:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/kubelet.json
        token: ''
      pvc:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/pvc.json
        token: ''
    grafana-dashboards-cilium:
      cilium:
        url: https://raw.githubusercontent.com/eltorio/oci-manage/main/miscellaneous/dashboards/cilium.json
        token: ''
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: LocalCluster
        type: prometheus
        url: http://prometheus-kube-prometheus-prometheus:9090
        access: proxy
        isDefault: true
EOF
  helm upgrade --install --namespace $MONITORING_NAMESPACE prometheus prometheus-community/kube-prometheus-stack --values /tmp/$UUID/values.yaml
  rm -rf /tmp/$UUID
}

function cluster_runners_clean() {
  cluster_run_on_all_as_root "rm -rf /storage/github-runner"
  kubectl delete namespace oci-runners
  cluster_docker_prune
}
function cluster_top() {
  cluster_run_on_all_as_current_user "top -b -n 1 | head -n 20"
}

function cluster_install_stressng() {
  cluster_copy_file_as_root miscellaneous/stress-ng.service /lib/systemd/system/
  cluster_run_on_all_as_root "sudo systemctl daemon-reload && sudo systemctl start stress-ng"
}

function cluster_stressng() {
  cluster_run_on_all_as_root "sudo systemctl $1 stress-ng.service"
}

function _cluster_test_get_nc_tcp_singleport_test_ipv4() {
  echo "echo -e \"*******TESTING IPv4 PORT $2\" && nc -4 -zv $1 $2"
}

function _cluster_test_get_nc_tcp_singleport_test_ipv6() {
  echo "echo -e \"*******TESTING IPv6 PORT $2\" && nc -6 -zv $1 $2"
}

function _cluster_test_get_nc_tcp_singleport_test_ip() {
  echo "echo -e \"*******TESTING IPv6 PORT $2\" && nc -zv $1 $2"
}

function cluster_test_generate_remote_test() {
  for MEMBER in $K8S_CLUSTER; do
    echo "echo -e \"********************$MEMBER.$DNS_PUBLIC_ZONE******************\\n*******TESTING HTTPS CONNECT\""
    echo "echo QUIT | openssl s_client -quiet -connect $MEMBER.$DNS_PUBLIC_ZONE:443"
    echo "$(_cluster_test_get_nc_tcp_singleport_test_ipv4 $MEMBER.$DNS_PUBLIC_ZONE 22)"
    echo "$(_cluster_test_get_nc_tcp_singleport_test_ipv4 $MEMBER.$DNS_PUBLIC_ZONE 6922)"
    echo "$(_cluster_test_get_nc_tcp_singleport_test_ipv4 $MEMBER.$DNS_PUBLIC_ZONE 43683)"
    echo "$(_cluster_test_get_nc_tcp_singleport_test_ipv6 $MEMBER.$DNS_PUBLIC_ZONE 22)"
    echo "$(_cluster_test_get_nc_tcp_singleport_test_ipv6 $MEMBER.$DNS_PUBLIC_ZONE 6922)"
    echo "$(_cluster_test_get_nc_tcp_singleport_test_ipv6 $MEMBER.$DNS_PUBLIC_ZONE 43683)"
    echo "echo -e \"\n\n\n\""
  done
}

function cluster_test_get_external_ip() {
  for MEMBER in $K8S_CLUSTER; do
    dig +short $MEMBER.$DNS_PUBLIC_ZONE A
  done
  for MEMBER in $K8S_CLUSTER; do
    dig +short $MEMBER.$DNS_PUBLIC_ZONE AAAA
  done
}

function cluster_test_get_external_ip_ha() {
  dig +short ha.$DNS_PUBLIC_ZONE A
  dig +short ha.$DNS_PUBLIC_ZONE AAAA
}

function cluster_test_external_connection() {
  PORTS="22 443 6922 43683"
  for IP in $(cluster_test_get_external_ip); do
    for PORT in $PORTS; do
      echo "$(_cluster_test_get_nc_tcp_singleport_test_ip $IP $PORT)"
    done
  done
}

function cluster_test_external_connection_ha() {
  PORTS="22 443 6922 43683"
  for IP in $(cluster_test_get_external_ip_ha); do
    for PORT in $PORTS; do
      echo "$(_cluster_test_get_nc_tcp_singleport_test_ip $IP $PORT)"
    done
  done
}

function cluster_test_external_connection_remote() {
  PORTS="22 443 6922 43683"
  for IP in $(cluster_test_get_external_ip); do
    for PORT in $PORTS; do
      nc -zv $IP $PORT
    done
  done
}

function cluster_clean_syslog_old() {
  cluster_run_on_all_as_root "rm -rf /var/log/*.gz && truncate -s 1M /var/log/*.1"
}

function cluster_longhorn_patch_pvc_error() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_longhorn_patch_pvc_error pvc_id"
  else
    kubectl -n longhorn-system patch lhsm $1 --type=merge --subresource status --patch 'status: {state: error}'
  fi
}

function cluster_deployment_action() {
  if [ "$#" -ne 3 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_deployment_action pause/stop/resume namespace deployment_name"
  else
    kubectl rollout -n $2 $1 deployment/$3
  fi
}

function cluster_deployment_pause() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_deployment_pause namespace deployment_name"
  else
    clustert_deployment_action pause $1 $2
  fi
}

function cluster_deployment_resume() {
  if [ "$#" -ne 2 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_deployment_resume namespace deployment_name"
  else
    clustert_deployment_action resume $1 $2
  fi
}

function cluster_untaint_disk_pressure_all() {
  for MEMBER in $(kubectl get nodes -ojson | jq -r '.items[] | .metadata.name '); do
    kubectl taint nodes $MEMBER node.kubernetes.io/disk-pressure-
  done

}
function cluster_memory_free_size() {
  cluster_run_on_all_as_root "free -hm | awk 'NR==2{print \$7}'"
}
function cluster_nodefs_free_size() {
  cluster_run_on_all_as_root "df -h / | awk 'NR==2{print \$5}'"
}
function cluster_imagefs_available_free_size() {
  cluster_run_on_all_as_root "df -h /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs | awk 'NR==2{print \$5}'"
}
function cluster_docker_prune() {
  cluster_run_on_all_as_root "docker volume prune  --force ; docker images prune ; docker system prune --force ; crictl -r unix:///run/containerd/containerd.sock rmi --prune"
}
function cluster_docker_install_qemu() {
  cluster_run_on_all_as_current_user "docker run -it --rm --privileged tonistiigi/binfmt --install amd64"
}

function cluster_minio_install_mc() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_minio_install_mc host"
  else
    ssh root@$1 'curl https://dl.min.io/client/mc/release/linux-$(dpkg --print-architecture)/mc -o /usr/local/bin/mc && chmod +x /usr/local/bin/mc'
  fi
}

function cluster_minio_install_mc_all() {
  for MEMBER in $K8S_CLUSTER; do
    echo "********************$MEMBER******************"
    cluster_minio_install_mc $MEMBER
  done
}

function cluster_etcd_install_binaries() {
  mkdir -p $HOME/etcd &&
    curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest |
    grep browser_download_url | grep linux-$(dpkg --print-architecture) | cut -d '"' -f 4 | wget -qi - -O - |
      tar -xvz --strip-components=1 -C $HOME/etcd &&
    sudo mv $HOME/etcd/etcdutl $HOME/etcd/etcdctl $HOME/etcd/etcd /usr/local/bin/ &&
    rm -rf $HOME/etcd
}

function cluster_etcd_snapshot() {
  KUBERNETES_CONTROL_PLANE=$(kubectl get nodes --selector node-role.kubernetes.io/control-plane -o jsonpath='{.items[*].metadata.name}')
  BASE_BACKUP_DIR=$(pwd)/etcd-snapshots
  for node in $KUBERNETES_CONTROL_PLANE; do
    echo "Processing node: $node"
    ssh root@$node "ETCDCTL_API=3 etcdctl snapshot save snapshot.db --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key"
    ssh root@$node "tar -cv snapshot.db  /usr/lib/systemd/system/kubelet.service  /usr/lib/systemd/system/kubelet.service.d /etc/haproxy/haproxy.cfg /etc/kubernetes/pki /etc/kubernetes/manifests /etc/kubernetes/*.conf | xz -9 | openssl enc -aes-256-cbc -base64 -md sha256 -pass pass:'$CRYPTOKEN' > $BASE_BACKUP_DIR/backup.tar.xz.enc"
    ssh root@$node "rm -rf snapshot.db"
    scp root@$node:$BASE_BACKUP_DIR/backup.tar.xz.enc $BASE_BACKUP_DIR/backup-$node.tar.xz.enc
    ssh root@$node "rm -rf $BASE_BACKUP_DIR/backup.tar.xz.enc"
  done
}

function cluster_etcd_get_member_list() {
  sudo ETCDCTL_API=3 etcdctl --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list
}

function cluster_etcd_save_backup_s3() {
  sudo ETCDCTL_API=3 etcdctl $ETCDCTL_PKI defrag --cluster
  cluster_etcd_snapshot
  KUBERNETES_CONTROL_PLANE=$(kubectl get nodes --selector node-role.kubernetes.io/control-plane -o jsonpath='{.items[*].metadata.name}')
  for node in $KUBERNETES_CONTROL_PLANE; do
    echo "Processing node: $node"
    cp etcd-snapshots/backup-$node.tar.xz.enc etcd-snapshots/etcd-backup-$(date --iso-8601).tar.xz.enc
    mc cp etcd-snapshots/backup-$node.tar.xz.enc $S3PATH/etcd-backup-$(date --iso-8601).tar.xz.enc
    rm etcd-snapshots/etcd-backup-$(date --iso-8601).tar.xz.enc
  done
}

# crypto_cat is a function to decrypt a file with openssl
# it uses automatically the CRYPTOKEN variable as password
# use the stdout to pipe it to another command
function crypto_cat() {
  DECRYPT=""
  if [ -n "$CRYPTOKEN" ]; then
    DECRYPT="-pass pass:$CRYPTOKEN"
  else
    DECRYPT=""
  fi
  openssl aes-256-cbc -a -d -md sha256 $DECRYPT -in $1 -out -
}

function cluster_apt_update_kubernetes_repo() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_apt_update_kubernetes_repo version (ie: 1.29)"
  else
    VERSION=$1
    cluster_run_on_all_as_root "echo 'deb [arch=arm64 signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$VERSION/deb/ /' > /etc/apt/sources.list.d/kubernetes.list && apt update"
  fi
}

function cluster_master_allow_schedule() {
  CONTROL_PLANES=$(kubectl get nodes | grep control-plane | awk '{print $1}')
  for CONTROL in $CONTROL_PLANES; do
    kubectl taint node $CONTROL node-role.kubernetes.io/control-plane:NoSchedule-
  done

}

function git_prune_local_repo() {
  git fetch --depth=1 && git reflog expire --expire-unreachable=now --all && git gc --aggressive --prune=all
}

function cluster_reboot_instant() {
  if [ "$#" -ne 1 ]; then
    echo "Illegal number of parameters:" && echo "    usage cluster_reboot_instant member"
  else
    MEMBER=$1
    ssh root@$MEMBER "echo 1 > /proc/sys/kernel/sysrq && echo b > /proc/sysrq-trigger"
  fi
}

function cluster_reboot_instant_all() {
  for MEMBER in $K8S_CLUSTER; do
    echo "********************$MEMBER******************"
    cluster_reboot_instant $MEMBER
  done
}

function get_helm() {
  curl -fsSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
}

# This function generates a random IPv6 address compliant with RFC4193.
# RFC4193 addresses are local IPv6 addresses with a prefix of fd or fc.
# The function first generates a random prefix of fc or fd.
# Then, it generates a Local Unicast Identifier (LUI) which is a 40-bit random number.
# The prefix and the LUI are combined to form a 48-bit IPv6 address.
# The function then formats this address by inserting ':' every four characters.
# Finally, it removes the trailing ':' and appends '::/48' to indicate a 48-bit network prefix.
function get_random_ipv6_rfc4193() {
  #PREFIX=f$((( RANDOM % 2 )) && echo -n "c" || echo -n "d")
  PREFIX=fd
  LUI=$(openssl rand -hex 5)
  CIDR=$(echo -n $PREFIX$LUI | sed 's/\(....\)/\1:/g')
  CIDR=${CIDR::-1}
  echo $CIDR::/48
}

cluster_control_plane_renew_certificates(){
  CONTROL_PLANES=$(kubectl get nodes | grep control-plane | awk '{print $1}')
  for CONTROL in $CONTROL_PLANES; do
    echo "********************$CONTROL******************"
    ssh root@$CONTROL "kubeadm certs renew all && systemctl restart kubelet"
  done
}

function cluster_help() {
  echo 'azure_install_cli:                                 install azure cli'
  echo 'cluster_apt_dist_upgrade:                          apt dist-upgrade on all the cluster'
  echo 'cluster_apt_install:                               install an apt package on all the cluster'
  echo 'cluster_apt_update_kubernetes_repo:                update the Kubernetes major release in the apt source llist'
  echo 'cluster_apt_list_upgradable:                       list all available upgrades'
  echo 'cluster_cilium_full_status:                        issue cilium status on each member'
  echo "cluster_cilium_install:                            install cilium (or upgrade with argument upgrade)"
  echo 'cluster_cilium_hostip_all:                         get all cilium pods IPP'
  echo 'cluster_cilium_hostip_cross_ping:                  ping all cilium interfaces'
  echo 'cluster_cloudflare_external_dns:                   create external dns for cloudflare dns 'needs CF_API_KEY and CF_API_EMAIL''
  echo 'cluster_control_plane_renew_certificates:          renew all control plane certificates'
  echo 'cluster_copy_dir_as_root:                          copy local dir to remote dir (take care of the trailing slashes)'
  echo 'cluster_copy_file_and_run_as_root:                 copy local file as root to each member and runs it remotely'
  echo 'cluster_copy_file_as_current_user:                 copy local file to each member'
  echo 'cluster_copy_file_as_root:                         copy local file as root to each member'
  echo 'cluster_coredns_add_k8s_external:                  add cluster.external/$EXTERNAL_CLUSTER_ZONE zone to coredns wich resolve all the services'
  echo 'cluster_create_ingress_cloudflare:                 Create an ingress with letsencrypt certificate and create a CNAME on cloudflare'
  echo 'cluster_create_local_registry_secret:              create a secret named local-registry-credential containing login to local registry'
  echo 'cluster_delete_node:                               delete a k8s node'
  echo 'cluster_delete_stucked_namespace:                  delete a namespace stucked on terminating (perhaps leaving dangling resources)'
  echo 'cluster_delete_stucked_pods:                       delete pods in a spoecified namespace stucked in terminatig state'
  echo 'cluster_delete_stucked_pvc:                        delete stucked pvc/pc (you probably need to run it twice)'
  echo 'cluster_deploy_ca_cert                             install ca from ROOT_CA in the Ubuntu trust store'
  echo 'cluster_deploy_haproxy_config_on_members:          copy local /etc/haproxy/haproxy.cfg on each member and issuer a haproxy restart'
  echo 'cluster_deploy_hosts:                              create a /etc/hosts on each cluster member'
  echo 'cluster_deployment_action:                         stop/restart/resume… deployment'
  echo 'cluster_deployment_resume:                         resume deployment'
  echo 'cluster_deployment_pause:                          pause deployment'
  echo 'cluster_docker_prune:                              recover some orphaned on docker'
  echo 'cluster_docker_install_qemu:                       install needs for buildkit'
  echo 'cluster_delete_member:                             delete gracefully a member from the cluster'
  echo "member_enable_sysctl_value:                        set to 1 a arbitrary sysctl.conf value"
  echo "member_disable_sysctl_value:                       set to 0 a arbitrary sysctl.conf value"
  echo "cluster_enable_sysctl_value:                       set to 1 a arbitrary sysctl.conf value"
  echo "cluster_disable_sysctl_value:                      set to 0 a arbitrary sysctl.conf value"
  echo 'cluster_enable_ipv4_forward:                       set net.ipv4.ip_forward=1'
  echo "cluster_enable_ipv6_forward:                       set net.ipv6.conf.all.forwarding=1"
  echo "cluster_enable_bridge_nf_call_iptables:            set net.bridge.bridge-nf-call-iptables=1"
  echo "cluster_enable_bridge_nf_call_ip6tables:           set net.bridge.bridge-nf-call-ip6tables=1"
  echo "cluster_etcd_install_binaries:                     install latests etcd binaries in /usr/local/bin"
  echo "cluster_etcd_snapshot:                             create and encrypt a etcd snapshot (pass=$CRYPTOKEN)"
  echo "cluster_etcd_get_member_list:                      get the etcd member list"
  echo "cluster_etcd_save_backup_s3:                       transfer the last etcd backup to  $S3PATH/etcd-backup-$(date --iso-8601).tar.xz.enc"
  echo 'cluster_get_all_public_ip:                         retrieve all IPv4 and IPv6 of each members by querying $MEMBER.$CLUSTER_DOMAIN'
  echo "cluster_coredns_get_corefile:                      retrieve the coredns Corefile"
  echo "cluster_get_external_ips:                          retrieve all external addresses of the cluster"
  echo "cluster_get_host_external_ipv4or6:                 retrieve the external ipv4 (with A) or ipv6 (with AAAA) host address"
  echo "cluster_get_traefik_lb_ip:                         get the traefik load balancer external ip"
  echo "cluster_haproxy_get_validation_script:             get a script for validating haproxy-ingress, copy it and run it from outside, all members should answer 'Not FoundNot Found'"
  echo "cluster_init_azure_dns_issuer:                     install letsencrypt-azure and letsencrypt-staging-azure clusterissuer against Azure DNS zone"
  echo "cluster_init_cilium_hubble_ui_ingress:             create hubble-ui ingress with same auth as traefik dashboard"
  echo "cluster_init_coredns_config:                       create the initial coredns config with cluster.external zone"
  echo "cluster_init_create_control_plane_pool_kubernetes: create control plane with ipam=kubernetes"
  echo "cluster_init_create_control_plane:                 create control plane with ipam=cluster-pool"
  echo "cluster_init_create_master_haproxy_cfg:            create a basic haproxy 443/tcp proxy to traefik"
  echo "cluster_init_create_another_control_plane:         create another control plane"
  echo "cluster_init_create_member:                        join a unique member to k8s cluster given its name"
  echo "cluster_init_create_members:                       join all members to k8s cluster"
  echo "cluster_init_create_post_install:                  run post install deployement (certmanager, traefik, dashboard)"
  echo "cluster_init_create_post_install_grafana_v3:       deploy cloud grafana"
  echo "cluster_init_create_post_install_prometheus:       deploy local prometheus/grafana stack"
  echo "cluster_init_install_certmanager:                  deploy cert-manager with ca"
  echo "cluster_init_install_cri_containerd:               install cri-containerd"
  echo "cluster_init_install_cri_docker:                   install cri_docker on arm64 only"
  echo "cluster_init_install_dashboard:                    deploy dashboard"
  echo "cluster_init_install_kubevirt:                     Install latest Kubevirt Arm64"
  echo "cluster_init_install_longhorn_ingress:             install longhorn ingress for all LONGHORN_DASHBOARD_DNS_NAMES"
  echo "cluster_init_install_longhorn:                     install longhorn storage management"
  echo "cluster_init_install_oci_dns_issuer                install letsencrypt-oci and letsencrypt-staging-oci clusterissuer against OCI DNS zone"
  echo "cluster_init_install_openebs:                      install openEbs storage management"
  echo "cluster_init_install_traefik:                      deploy the traefik helm chart in kube-traefik namespace"
  echo "cluster_init_sysctl_value:                         set the needed sysctl values"
  echo "cluster_install_haproxy_2_7:                       install haproxy 2.7 and haproxy-ingress external"
  echo "cluster_install_haproxy_2_7_single:                install haproxy 2.7 and haproxy-ingress external on single node"
  echo "cluster_install_helm_dashboard_ingress:            create an ingress for the helm dashboard"
  echo "cluster_install_helm_dashboard:                    install the helm dashboard"
  echo "cluster_install_qemu_x86_64_for_build:             cluster_install_qemu_x86_64_for_build"
  echo "cluster_install_stressng:                          deploy stress-ng everywhere for simulating load"
  echo "cluster_install_weblate:                           install or upgrade if param is upgrade weblate"
  echo "cluster_iptables_flush:                            flush iptables on control plane and all members"
  echo "cluster_iptables_open_ipv4_port:                   open an ipv4 on all the cluster firewalls"
  echo "cluster_lb_start:                                  start lb service"
  echo "cluster_limit_syslog_to_10MB:                      limit the size of the syslog"
  echo "cluster_kubernetes_proxy_test:                     check for connection to 172.24.0.1"
  echo "cluster_lb_stop:                                   stop lb service"
  echo "cluster_longhorn_patch_crd_finalizers:             remove all finalizers from Longhorn custom resource definitions (for allowing deleting)"
  echo "cluster_longhorn_patch_pvc_error:                  workaround longhorn #7183 on bug 1.5.3"
  echo "cluster_longhorn_upgrade:                          upgrade longhorn"
  echo "cluster_master_allow_schedule:                     Allow scheduling on control-plane"
  echo "cluster_memory_free_size:                          get free memory on each nodes"
  echo "cluster_minio_install_mc:                          install minio cli on specified member"
  echo "cluster_minio_install_mc_all:                      install minio cli on all members"
  echo "cluster_imagefs_available_free_size:               get imagefs available on each nodes"
  echo "cluster_needrestart:                               run needrestart on all members"
  echo "cluster_nodefs_free_size:                          get free nodefs"
  echo "cluster_ping_host_from_members:                    ping the given host from each member"
  echo "cluster_ping6_host_from_members:                   ping the given host from each member with IPv6"
  echo "cluster_reboot:                                    reboot all the cluster"
  echo "cluster_reboot_instant:                            reboot a member instantly"
  echo "cluster_reboot_instant_all:                        reboot all the cluster instantly"
  echo "cluster_recreate_private_interface:                recreate all private interface netplan yaml"
  echo "cluster_remove_completed_pods:                     remove all completed pods using a field selector"
  echo "cluster_reset_control_plane:                       delete the control plane"
  echo "cluster_reset_member:                              reset one member (without leaving the cluster)"
  echo "cluster_reset_members:                             delete all members"
  echo "cluster_reset_full:                                full delete"
  echo "cluster_reset_remove_azure_dns_issuer:             remove the letsencrypt azure issuers"
  echo "cluster_reset_remove_kubevirt:                     uninstall kubevirt"
  echo "cluster_reset_remove_oci_dns_issuer:               remove the OCI Letsencrypt certificate issuers"
  echo "cluster_reset_storage:                             empty all /storage"
  echo "cluster_haproxy_k8s_status:                        status haproxy for k8s high availability"
  echo "cluster_haproxy_restart:                           restart haproxy"
  echo "cluster_haproxy_status:                            status haproxy"
  echo "cluster_haproxy_start:                             start haproxy"
  echo "cluster_haproxy_stop:                              stop haproxy"
  echo "cluster_run_on_all_as_current_user:                run as current user on all the cluster"
  echo "cluster_run_on_all_as_root:                        run as root on all the cluster"
  echo "cluster_run_on_all_members_as_current_user:        run a command on each member as root"
  echo "cluster_run_on_all_members_as_root:                run a command on each member"
  echo "cluster_runners_clean:                             remove runners"
  echo "cluster_stressng:                                  use start restart status or stop for managing stress-ng daemon"
  echo "cluster_test_get_external_ip:                      get all published ip on $DNS_PUBLIC_ZONE"
  echo "cluster_test_external_connection:                  test access from exterior on all ips"
  echo "cluster_test_external_connection_remote:           copy answer on other machine for testing access"
  echo "cluster_test_external_connection_ha:               copy answer on other machine for testing access"
  echo "cluster_test_tcp_port:                             open a tcp server and try to reach it"
  echo "cluster_test_udp_port:                             open a udp server and try to reach it"
  echo "cluster_test_generate_remote_test:                 generate tests to copy/paste on a remote host for testing connectivity"
  echo "cluster_top:                                       see the first 20 lines of top on each member"
  echo "cluster_traefik_launch_simple_proxy:               lauche simpleproxy from private ip to traefik loadbalancer"
  echo "cluster_update_k8s_pause:                          update registry.k8s.io/pause to the given version"
  echo "cluster_untaint_disk_pressure_all:                 untaint all disk pressure"
  echo "crypto_cat:                                        decrypt a file with openssl"
  echo "create_admin_user_with_key:                        create an admin user with its home directory and a ssh key"
  echo "dashboard_get_token:                               retrieve dashboard administrator token"
  echo "dev_install_local_registry                         create a local docker registry without pvc for developpement (docker-registry.local)"
  echo "dev_local_registry_garbage_collect:                run garbage-collect on dev registry"
  echo "dev_local_registry_get_podname:                    retrieve dev local registry pod name"
  echo "get_gateway_from_ip:                               get gateway for ip"
  echo "get_gateway:                                       get gateway for named host"
  echo "get_helm:                                          install helm"
  echo "get_host_arch:                                     display the debian host arch"
  echo "get_kube_dns_ip:                                   compute kube-dns internal ip for Service CIDR"
  echo "get_random_ipv6_rfc4193:                           generate a random IPv6 address compliant with RFC4193"
  echo "get_remote_interface_from_ip:                      get the remote interface corresponding to ip"
  echo "get_remote_interface:                              get the remote interface on a host giving its mac address"
  echo "get_subnet_16:                                     get full subnet"
  echo "get_subnet_from_ip:                                get subnet for ip"
  echo "get_subnet:                                        get subnet for named host"
  echo "git_prune_local_repo:                              prune local git repo"
  echo "init_allow_keys_for_root:                          allow all admins keys in the new host"
  echo "init_create_admin_user_with_key:                   create an admin user on a new host"
  echo "init_create_private_interface:                     create the private interface"
  echo "init_install_cri_containerd:                       disable cri-dockerd and enable cri-containerd"
  echo "init_install_cri_docker:                           deploy the cri_docker on the new host"
  echo "init_install_software:                             install and uptate initial sofware for runnig cluster"
  echo "init_set_hostname:                                 set the hostname of the new member"
  echo "init_set_iptables_members:                         set iptables an all members"
  echo "init_set_iptables:                                 deploy the needed iptables on the new host"
  echo "oci_get_all_tenancy_compartment_triplet:           get all tenancy and compartment ocid from oci.json"
  echo "oci_action_instance:                               execute an action (ex RESET) on one instance via oci api"
  echo "oci_action_all_instances:                          execute an action (ex RESET) on all instance via oci api"
  echo "oci_generate_instance_list:                        regenerate oci-tools/src/config/instances.json"
  echo "oci_list_compartment:                              list all the tenancy compartments"
  echo "oci_list_instance_running:                         list all running instances given a tenancy and a compartment ocid"
  echo "oci_list_vcn:                                      get the vcn given a tenancy and a compartment ocid"
  echo "oci-manage-copyright:                              shows copyright"
  echo "wg_meshconf_addpeer:                               add wireguard peer"
  echo "wg_meshconf_apt_dist_upgrade:                      Launch apt update && apt -y dist-upgrade on each wireguard peers"
  echo "wg_meshconf_deploy_config:                         generate and deploy wireguard mesh network"
  echo "wg_meshconf_get_all_peers_sorted:                  parse and sort the wireguard csv db"
  echo "wg_meshconf_init:                                  initialise the wg meshconf database"
  echo "wg_meshconf_showpeers:                             list all peers in wireguard mesh network"
  private_help
}
